{"meta":{"title":"开普勒鑫球","subtitle":null,"description":"dapper talos","author":"开普勒鑫球","url":"http://kplxq.github.io"},"pages":[{"title":"关于我们","date":"2018-01-06T09:48:39.942Z","updated":"2018-01-06T09:48:39.942Z","comments":true,"path":"about/index.html","permalink":"http://kplxq.github.io/about/index.html","excerpt":"","text":"开普勒鑫球是一个开放式的技术团体，是新金融科技的先行者。我们将探索金融科技发展，用新科技打造新金融，通过技术孵化、知识分享、项目开源，与业界同行共进。 开源项目 Talos github gitee QQ交流群：637375352 微信群：请联系管理员加入 微信公众号： 官方社区：www.kplxq.com"},{"title":"Categories","date":"2017-12-21T14:20:50.796Z","updated":"2017-12-21T14:20:50.796Z","comments":true,"path":"categories/index.html","permalink":"http://kplxq.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2017-12-21T14:20:50.796Z","updated":"2017-12-21T14:20:50.796Z","comments":true,"path":"tags/index.html","permalink":"http://kplxq.github.io/tags/index.html","excerpt":"","text":""},{"title":"开鑫金服招聘","date":"2018-02-26T13:43:20.483Z","updated":"2018-02-26T13:43:20.483Z","comments":true,"path":"jobs/index.html","permalink":"http://kplxq.github.io/jobs/index.html","excerpt":"","text":"这么厉害！不亏是大神，你竟然发现了这个页面。既然咱们这么有缘，就来领取给你的福利吧： 招聘 Java开发工程师（测试MM干活时，负责在旁剥瓜子） 测试工程师（开发GG干活时，负责在旁加油） 弹性工作制、开放的工作氛围、一群优秀活泼的小伙伴、一个来了就不想走的工作机会…… 快拿起你手中的电话或邮箱，跟我们联系吧。邮箱：job@kxjf.com电话：025-52817677联系人：漂亮的杨MM who we are 开鑫金服是江苏省金融办批准、国家开发银行发起设立的互联网金融服务集团，互金行业标杆企业，平台累计成交已超1000亿元！引领创新，稳健前行！ 我们还是中国互联网金融协会常务理事单位，受到江苏省人民政府重点支持。 “支持实体 服务百姓”是我们一直坚持的经营理念。 Java开发工程师一、岗位职责1、测试MM干活时，负责在旁剥瓜子。2、对互联网金融系统的模块进行设计，并承担系统较复杂模块的编码，修复测试发现的问题；3、针对项目目标分解开发任务，并能安排5人以下团队的开发计划，推进计划完成；4、开发过程中，对团队成员进行技术指导，并推进开发规范落实；5、对团队成员的成果（代码、文档）进行评审，使其符合质量控制要求；6、监测系统模块在产线的运行状况，定位并处理系统出现的严重性故障。二、任职要求1、计算机相关专业，本科及以上学历； 3年以上互联网/企业级软件开发经验，3年以上J2EE/前端开发经验，1年以上系统设计经验；2、 掌握软件开发基础知识，包括软件工程、面向对象、数据结构、数据库原理等；熟悉团队管理、研发项目管理流程、软件开发规范；3、掌握软件需求分析、系统设计、设计模式相关知识；掌握开发工作常用的工具，包括文档、画图、系统设计、IDE、数据库等；4、掌握J2EE/前端开发语言及平台自带常用类库，包括Java、JavaScript、HTML、CSS等；掌握J2EE/前端常用技术框架，包括：Spring、Spirng MVC、Mybatis、Freemarker、Tomcat、JQuery等。5、了解互联网产品用户体验相关的知识；了解质量保障体系的相关知识；掌握挖掘系统问题及系统化分析与解决问题的相关理论及方法；6、有互联网金融开发背景者，参与大型项目设计或开发工作者优先。 测试工程师一、岗位职责：1、开发GG干活时，负责在旁加油。2、参与需求讨论、制定测试策略和构建测试环境；3、根据需求文档、设计文档分析测试点、测试范围，设计和编写手动和自动测试用例；4、编写和维护自动化测试脚本5、协助定位bug，配合开发人员重现和修复bug；二、职位要求：1、本科或以上学历，计算机相关专业；2、具有2年以上软件测试或开发经验,熟悉软件测试流程和测试方法；3、具有自动化测试开发经验;4、具有高度的责任心和较高的质量意识，良好的沟通协作意识，较强的问题解决能力；5、熟悉python、java者优先"}],"posts":[{"title":"穹顶之下，我们在哪里？","slug":"穹顶之下，我们在哪里？","date":"2018-05-14T12:18:31.000Z","updated":"2018-05-15T12:30:53.099Z","comments":true,"path":"2018/05/14/穹顶之下，我们在哪里？/","link":"","permalink":"http://kplxq.github.io/2018/05/14/穹顶之下，我们在哪里？/","excerpt":"城市日益璀璨的华灯让天空越来越高，也让人逐渐迷失了自我，我们在哪里？穹顶之下，你我不过沧海一粟。让我们给这繁忙的生活按下暂停键，仰望头顶这方浩瀚星空，寻找不期而遇的惊喜，聆听来自宇宙的心思。","text":"城市日益璀璨的华灯让天空越来越高，也让人逐渐迷失了自我，我们在哪里？穹顶之下，你我不过沧海一粟。让我们给这繁忙的生活按下暂停键，仰望头顶这方浩瀚星空，寻找不期而遇的惊喜，聆听来自宇宙的心思。 写在前面夜深人静，折腾了一天的代码终于可以开始全部编译。伸个懒腰，离开已经被坐塌的旋转椅，走上阳台，仰望星空，有没有想过或许在遥远的仙女座星系，也有个同样在加班的程序员，正在用同样的眼光注视着你？ 浩瀚星空，沧海一粟。随着生活水平的日益提高，越来越多的码农已经不满足于把每天24小时的生活全部花在关注地球上的Java代码是否能够稳定运行，而是开始对遥远的三体人产生了浓厚的兴趣。天文观测除了能够为我们带来无尽的遐想之外，能够亲自上阵，手把手把理论变成实践，站在星空下找寻天体的乐趣，寻找那不期而遇的惊喜，也是一种难得的享受。 由于本人水平有限，万千头绪不知从何说起，思前想后，还是从我们身处宇宙何处来首先感受一下宇宙的魅力。从目前的理论看，宇宙是没有“边界”的，因此我们无法标出我们在宇宙中的绝对位置，本文所提到的位置均为基于可观测宇宙，地球位于可观测宇宙的中心。 地球 说到我们身处何处，全地球人都知道，我们当然是身处地球，一颗拥有45.4亿年历史的行星，人类已知宇宙中唯一存在生命的天体，想观测三体人的可以醒醒了。 地球的直径约为12700公里，如果有一条穿过地心的能够行走的高速公路，按照目前国内高速公路的普遍限速标准，没日没夜的开车，抵达地球的另一端大概需要106小时。 地球的平均轨道速度约为29.78km/s，轨道周期约为一年。问题来了，地球的轨道中心是什么？知道答案的可以留言，有惊喜哦~ 太阳系 没错，太阳系就是我们初中地理（还是小学自然课？）中提到的当年拥有9大行星的那个行星系统了。为什么是当年，因为冥王星因不满足国际天文联合会(IAU) 2006年关于行星的天体定义而被开除资格了 IAU于2006年列出决议，符合以下三个条件的天体可被视为太阳系体内的行）： 该天体的轨道必须围绕太阳运转； 该天体必须有足够的质量通过自身引力成为球形； 该天体必须清理轨道附近的其他天体 冥王星因为体重不够，被归属到了矮行星的队伍。 太阳系内天体除了行星、矮行星、恒星（太阳自己）外，还有小行星、彗星、卫星等。 太阳系的直径是4光年，也就是说光大概要走4年，按照阿尔伯特爷爷的狭义相对论理论，有兴趣的可以算算如果按照目前国内高速公路的普遍限速标准，没日没夜的开车大概要多长时间？ 相对于银河中心，太阳系的轨道速度约为220km/s，轨道周期约为2亿2千5百万至2亿5千万年。 银河系还记得小时候的鹊桥相会么？阴历7月初7的夜晚，牛郎织女一年一度的鹊桥小聚。先把牛郎、织女抛开不说，鹊桥，即是指喜鹊在银河上所搭之桥。 银河系是一个包含太阳系的棒旋星系，银河系内估计有1000亿到4000亿颗恒星，银河系直径约为10万到18万光年，地球所在的太阳系在距离银河中心26000光年的位置。把银河系想象为一个盛饭的碟子，从地球上看，因为是从碟子的内部向外观看，因此银河系呈现在天空中环绕一圈的带状。 本星系群 还有比银河系更大的玩意？少年，醒醒，飞出银河系你就想自由了？Naïve！我们来看看银河系在本星系群里面大概是个什么地位 本星系群，是包括地球所处银河系在内的一群星系。这组星系群包含大约超过50个星系，其质量中心位于银河系和仙女座星系之间的某处。本星系群直径大约为1000万光年 本星系群中比较常见的星系有仙女座星系（梅西耶编号M31，条件好的情况下肉眼可以观测）、三角座星系（M33），以及在南半球可见的大小麦哲伦星系 M31仙女座星系： 室女座超星系团 室女座超星系团又称本超星系团，包含银河系和仙女座星系所属的本星系群在内，至少有100个星系团与星系群聚集，直径约为1亿1千万光年，是在可观测宇宙中数以百万计的超星系团中的一个。 包括上面本星系群里面提到的仙女座、三角座，室女座是本文中提到的第三个星座。把整个宇宙想象为一个以地球为中心的球体，宇宙中所有的天体都投影在天球上，按照一定的规则将球面分为88个区域，即形成了天文学上的88个星座，这样宇宙中所有的天体都归属于某一特定的星座。等等，守护雅典娜的不是只有12宫么，怎么跑出来88个星座了？接着往下看。 将地球的公转轨道平面无限扩大，与天球相交形成黄道。黄道穿过天球上的13个星座，除了蛇夫座的一小部分之外，从春分点所在的双鱼座数起, 统称为黄道十二星座。而我们平时所熟悉的处女座，即上文中的室女座。 最新资料显示，我们的本星系群所在的超星系团是拉尼亚凯亚超星系团的一部分，中心是室女座星系团。 可观测宇宙 终于轮到boss出场了，由于宇宙还在不停的膨胀中，目前推测可观测宇宙直径约为930亿光。可观测宇宙大概包含上百万的超星系团。 写在最后好，懒腰伸完，代码编译完成! 恍惚中开始思考，如果某天人类征服了全宇宙，某宝的收货地址的后台代码是不是又要更改了，得支持 ’宇宙X-&gt;室女座超星系团-&gt;本星系群-&gt;银河系-&gt;太阳系-&gt;地球-&gt;中国-&gt;江苏省­-&gt;南京市-&gt;xxxxx’…… 注： 本文大部分图片来自404 wiki网站，如有涉及到版权问题，麻烦告知，立删 本文大部分知识来自404搜索网站，我不生产知识，我只是知识的搬运工。","categories":[],"tags":[{"name":"宇宙 科学","slug":"宇宙-科学","permalink":"http://kplxq.github.io/tags/宇宙-科学/"}],"keywords":[]},{"title":"基于STRIDE威胁建模的探索与实践（二）","slug":"基于STRIDE威胁建模的探索与实践（二）","date":"2018-05-10T12:18:16.000Z","updated":"2018-05-15T13:09:40.329Z","comments":true,"path":"2018/05/10/基于STRIDE威胁建模的探索与实践（二）/","link":"","permalink":"http://kplxq.github.io/2018/05/10/基于STRIDE威胁建模的探索与实践（二）/","excerpt":"屋里亲故啊，想死你们了，在上一篇中我们讲到：在互联网中，系统运行面临着很多的安全风险，一个软件系统应该满足哪些基本的信息安全需求以及有哪些安全设计原则。那么该篇将紧接上篇，讲解具体的应用实践。","text":"屋里亲故啊，想死你们了，在上一篇中我们讲到：在互联网中，系统运行面临着很多的安全风险，一个软件系统应该满足哪些基本的信息安全需求以及有哪些安全设计原则。那么该篇将紧接上篇，讲解具体的应用实践。 Threat Modeling威胁建模威胁建模的本质：通常我们无法证明给定的设计是安全的，但我们可以从既有的错误中汲取教训并避免犯同样的错误。 工欲善其事必先利其器，Microsoft威胁建模工具（Threat Modeling Tool）是一款基于STRIDE模型的威胁建模工具。通过该工具可以实现如下目的： 分析与交流系统的安全设计方案 使用经过证实的方法分析当前设计潜在的安全问题 提出和管理针对安全问题的解决措施 简而言之，威胁建模的全过程主要划分成4步，并且这4步将贯穿于SDL的安全流程管理之中： 构建模型图（Diagram）：安全需求阶段 识别威胁（Identify）：安全设计阶段 缓解问题（Mitigate）：安全实施阶段 验证（Validate）：安全测试阶段 构建模型图（安全需求阶段）Threat Modeling通过数据流关系图(DFD)来构建威胁模型，通过构建DFD，可以理清系统中重要的被保护对象以及业务边界，在研发需求阶段同步开展安全需求的整理。 工具自身提供了丰富的基础元件模板，以方便构建DFD，主要包含了6种类型： Generic Process（进程）：OS Process、Threat、Web Server等 Generic External Interactor（外部实体）：Browser、External Web Service、Megaservice等 Genetic Data Store（存储）：SQL Database、File System、Cache等 Genetic Data Flow（数据流）：HTTP、HTTPS、RPC or DCOM等 Genetic Trust Line Boundary（安全边界）：Internet Boundary、Machine Trust Boundary等 Genetic Trust Border Boundary（安全区域）：Internet Explorer Boundaries、Sandbox Trust Boundary Border等 基于上述6类模板元件，我们可以绘制出软件系统整体的交互需求图（威胁模型图），说白了就是将软件的需求以DFD数据流图的方式进行展现，该模型图画的越详细越准确，则最终分析得到的潜在威胁信息就越全面越准确，所以分析系统需求，构建威胁模型图是非常重要的。 下面以互金平台一个最简单的业务场景来构建威胁模型，基本的业务场景是：1、用户A通过浏览器操作互金网站B进行抢标投资操作；2、网站B创建完用户订单后，定向到网银C供用户进行相应的支付操作；3、最终网银C将支付结果反馈给网站B，网站B确认支付，业务结束。 绘制威胁模型图如下： 识别威胁（安全设计阶段）生成威胁列表Threat Modeling工具根据上面的威胁模型数据流图可以自动生成相应的Threat List（威胁列表），这些威胁列表均由工具基于STRIDE威胁模型生成，安全人员要做的就是对这些威胁列表进行识别与分析，找到威胁的原因。 威胁分析从截图中可以看出，Threat Modeling给出了几十条威胁告警,这些告警都是系统自动生的，但并非所有的威胁都是有效的、必要的，我们需要对这些告警进行安全分析确认，下面将挑选2条威胁进行示例分析。 威胁分析示例1 问题：该威胁属于“欺诈”类型的威胁，系统给出的解释是：“网站B可能会被黑客欺诈，同时用户A传递的信息也有可能会遭到信息泄露，建议采用一个标准化的安全认证机制进行信息交互”。 原因：经过安全人员的分析，系统做出这样判断的原因是：因为系统采用的通信协议为HTTP，是不安全的，因为该协议传递的是明文信息，这可能遭受到中间人（Attacker）窃听或篡改，这样对于用户A而言，他的信息遭到了泄露；对于网站B而言，它遭到了欺骗。 解决：采用Https协议进行通信。 威胁分析示例2 问题：该威胁属于“欺诈”类威胁。系统给出的解释是：由于网站B对于用户外部不受信任的输入未进行清理，所以网站B可能会被上传恶意脚本，进行CSS攻击。 原因：按照默认不信任原则，任意来自外部的请求都是可能存在风险的，网站B在处理这些输入的时候，需要进行必要的安全性校验，从而降低受到类似于CSS（XSS）、SQL-Injection等外部非法输入攻击的可能。 解决：网站B在WebFilter层进行敏感字符的过滤与校验，同时在页面渲染的时候对页面数据进行转义（HtmlEncode），以避免CSS、XSS等脚本攻击。 威胁报告Threat Modeling会基于构建的威胁模型生成可能存在潜在风险的威胁列表，安全人员只需要对照这些列表进行依次分析，同时对相应的威胁状态与安全等级进行标记或者调整，最终通过工具即可生成一份详细的威胁报告，威胁报告如下。 缓解问题（安全实施阶段）通过上述对威胁列表的分析与标记，我们最终得到了一份较为细致准确的威胁报告，接下来要做的就是与研发的程序猿们进行讨论，给出相应的安全修复建议，然后交于他们落地，进行相应的安全开发设计。 验证（安全测试阶段）安全人员可以基于威胁建模报告，输出相应的安全测试用例，在研发提交功能测试的同时，安全人员也将介入开展安全测试，从而验证威胁漏洞是否被很好的堵上，最终保障系统的安全。 反思通过Threat Modeling我们可以将DSL安全流程很好的引入到项目管理中，这对软件研发全流程安全的把控起到了积极的作用。 通过威胁建模，安全人员可以有效的分析出系统可能存在的安全漏洞，及早的给出研发人员安全修复建议，同时还可以指导自身安全测试工作的开展，这些对研发安全的效率与效果都具有良好的提升。 当然，安全不是一蹴而就的，安全的世界里充斥着太多的未知与变化，Threat Modeling提供的只是对已知威胁的分析，它可以帮助我们避免犯同样的错误，同时还有更多未知的威胁等待着我们去挖掘与探索。","categories":[],"tags":[{"name":"安全","slug":"安全","permalink":"http://kplxq.github.io/tags/安全/"}],"keywords":[]},{"title":"基于STRIDE威胁建模的探索与实践（一）","slug":"基于STRIDE威胁建模的探索与实践（一）","date":"2018-05-08T12:18:09.000Z","updated":"2018-05-15T13:09:29.908Z","comments":true,"path":"2018/05/08/基于STRIDE威胁建模的探索与实践（一）/","link":"","permalink":"http://kplxq.github.io/2018/05/08/基于STRIDE威胁建模的探索与实践（一）/","excerpt":"在互联网上，系统运行面临着很多的安全风险：假冒、篡改、抵赖、信息泄露、拒绝服务、权限提升等。那么如何应对这些风险呢？本文将和你一起探讨专注于软件开发安全保障的流程以及我们的应用实践。","text":"在互联网上，系统运行面临着很多的安全风险：假冒、篡改、抵赖、信息泄露、拒绝服务、权限提升等。那么如何应对这些风险呢？本文将和你一起探讨专注于软件开发安全保障的流程以及我们的应用实践。 微软SDL( Security Development Lifecycle)流程，是一种专注于软件开发安全保障的流程，该套流程贯穿于软件系统的整个研发生命周期，为软件的安全可靠提供了保障。 为此，我们引入了该流程，以保障系统研发的安全性。为了让SDL流程在项目管理中更好的推进实施，微软发布了一套威胁建模工具Threat Modeling Tool，依靠该工具我们可以很方便的将软件研发的需求、设计、测试、部署等阶段与SDL流程协同推进。 信息安全五大特征（软件安全需求） 通常情况下，软件系统必须满足以上5大基本的信息安全需求，在它的基础上进行相应的设计与研发，只有这样才能够保证软件交互过程各信息实体的安全性。说了这么多，到底什么样的软件设计是安全的？程序猿在系统设计的过程中要坚持哪些安全设计的原则呢？ 安全设计原则简单易懂原则：系统越复杂，越容易出错、越难审查透彻、越难测试全面，随着业务的复杂度增加，系统的安全风险也将逐渐递增。尽量不要为了装X，搞奇巧淫技去解决一个很Low的问题，这样其实是不对滴。 保护隐私原则隐私信息不再”裸奔”，需要加密。当前公民的个人信息贩卖已经形成了一条完整的地下黑产，信息泄露问题非常严峻。针对这一系列的问题，2017年实行的《国家网安法》中已经明确规定：网络运营者、网络产品或服务提供者以及关键信息基础设施运营者如未能依法保护公民个人信息，最高可被处以50万元罚款，甚至面临停业整顿、关闭网站、撤销相关业务许可或吊销营业执照的处罚。当前即使普通用户也已经意识到个人隐私就是安全问题，所以让用户的隐私信息在你的系统中不再“裸奔”，这即是客户的需求，更是法律的要求。 最薄弱环节保护原则保护最易受攻击影响的模块。比如，安全研发的时候需要重点考虑登录、注册、密码找回、更换手机号等安全事件易发多发模块。 纵深防御原则使用多重防御策略来管理风险，以便在一层防御被突破后，还存在另外一层防护阻止完全的破坏。这和银行除了有摄像头、安保人员，同时还具备防弹玻璃、电子门锁来维护安全是一个道理。比如企业级的Web应用，在网络层，除了构建基础防火墙以外，还会构建应用性更强的Web防火墙(WAF)，同时还会配合堡垒机等对系统进行安全审计、告警、定责；在应用层，则会设计信息加密、签名认证、前后端敏感信息双重过滤、防“安全绕过”服务器端业务逻辑校验等一系列的举措，从而最大限度的保证系统安全，达到多层防御的目的。 最小权限原则只授予执行操作所需的最小权限，并且对应访问权限只准许使用所需的最小时间。比如，用户登录网站超时后自动退出、投资用户只能查看有限的可投资标的信息，而无法查看全部的历史标的。 默认不信任程序世界里必须处处设防。人与人之间信任是非常重要的，但程序的世界里，可能信任越少越好，越是高性能高可用的系统，不信任原则会体现得更加淋漓尽致。比如，系统间的交互采用RSA算法签名认证的机制就是一种很好的体现。 故障安全化系统即使故障，那也要安全的着地。系统一旦出现故障，它将以安全方式失败，也就说系统安全是底线，最坏的结果是宕机服务关闭。 本章小结软件安全设计的原则其实还有很多，根据软件系统的差异化特征不尽相同，上述的这些属于最基本的安全原则。 安全人员需要针对自身系统分析总结更多的安全设计原则，提示研发人员在设计开发的过程中遵守这些原则，只有这样，软件系统的安全性才能从源头得到强有力的保障。 STRIDE威胁类型基于上文中提到的信息安全5大特征以及软件安全设计原则，为了确保应用程序具有这些安全属性，业界提出了STRIDE威胁类型，这些威胁类型基本上覆盖了常见的攻击手段。参考这些攻击手段，安全人员可以对系统进行整体的安全性分析。 STRIDE是以下6种威胁类型的英文首字母缩写： Spoofing(假冒) Tampering(篡改) Repudiation(抵赖) Information Disclosure（信息泄露） Denial of Service（拒绝服务） Elevation of Privilege（权限提升） 我们可以将这6类威胁类型与5大信息安全特性以及软件安全设计原则进行一次映射，如下： 从上面可以看出STRIDE威胁模型基本上可以覆盖软件安全的基础属性范畴，安全人员需要做的就是基于该威胁模型，对软件系统进行威胁建模。","categories":[],"tags":[{"name":"安全","slug":"安全","permalink":"http://kplxq.github.io/tags/安全/"}],"keywords":[]},{"title":"深入玩转Java多线程（一）Thread状态与生命周期","slug":"Thread状态与生命周期","date":"2018-05-03T12:17:50.000Z","updated":"2018-05-15T13:09:54.352Z","comments":true,"path":"2018/05/03/Thread状态与生命周期/","link":"","permalink":"http://kplxq.github.io/2018/05/03/Thread状态与生命周期/","excerpt":"问题：一个thread在start执行完后，还能再次start吗？ 123thread.start();TimeUnit.SECONDS.sleep(1);thread.start();","text":"问题：一个thread在start执行完后，还能再次start吗？ 123thread.start();TimeUnit.SECONDS.sleep(1);thread.start(); 连续调用两次start会异常Exception in thread “main” java.lang.IllegalThreadStateException原因是thread是有状态的，且有些状态之间的切换是不可逆的 Thread的状态 1、新建状态（New）：新创建一个线程对象的初始状态。2、就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的start()方法。该状态的线程位于JVM可运行线程池中，变得可运行。该状态下线程等待操作系统调度，获取CPU的使用权后就可以执行线程代码。3、阻塞状态（BLOCKED）：线程在执行过程中，遇到被synchronized关键字保护的代码，等待获得被保护对象的锁（waiting for a monitor lock），此时线程会停止执行。注意：只有synchronized这种方式的锁（monitor锁）才会让线程进入BLOCKED状态，等待利用cas实现的锁（如ReentrantLock）时，线程仍然处于Runnable状态。4、等待状态（WAITING or TIMED_WAITING）：让线程进入WAITING只有一种方式：在同步块中，调用锁对象的wait方法，也会让当前持有锁的线程进入wait状态。 123456789public static void timedWaiting() &#123; final Object lock = new Object(); synchronized (lock) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; &#125; &#125; &#125; 让线程进入TIMED_WAITING状态，可以给wait方法加一个入参超时时间，或者直接调用线程的thread.join()方法。处于WAITING状态的线程，只有其他线程调用了锁对象的notify方法才会执行。处于TIMED_WAITING的线程，除了被锁对象的notify方法唤醒外，到了超时时间会自动唤醒（被join方法阻塞的线程不会被自动唤醒，因为在底层实现中超时时间被设置为0）。5、死亡状态（Dead）：线程执行完了或者因异常退出了run()方法，该线程结束生命周期。 BLOCKED和WAITING状态的区别其实从字面意思就可以看出来：blocked是过去分词，意思是线程在执行过程中被别人卡住了，即其他线程正在执行这段由synchd保护的代码，等别人执行完自己就会自动执行（jvm调度控制），不需要其他线程唤醒。而waiting是主动执行wait动作后的当前状态，是主动卡住自己，必须由其他线程调用notify方法才能唤醒。从下面代码可以方便的理解：假设t1，t2先后两个线程，都执行如下代码：123synchronized(Obj) &#123; Obj.wait();&#125; t1先进，最后在Obj.wait()下卡住，这时java管t1的状态waitting状态t2后进，直接在第一行就卡住了，这时java叫t2为blocked状态。 Java处于方便管理的考虑，将blocked和waiting状态的线程放到两个队列里（BlockSet和WaitSet），当别的线程运行出了synchronized这段代码，jvm只需要去BlockSet里取，当某人调用了notify()，jvm只需要从WaitSet里取，后面锁机制的章节会详细介绍。所以两个状态的本质区别是进入和唤醒的条件不一样，其他没有任何区别。 Thread生命周期参考国外网站上一张图，个人认为画的最详细 注意：1、 Waiting、Block、Runnable之间可以两两切换，而TERMINATED就是终态了，永远不可能在启动了。2、 处于Runnable状态的线程，并不是立马就执行了，而是等待操作系统的调度启动。且线程再被CPU执行时也不是一口气执行到期，操作系统可能会将线程调出（比如有更高优先级的线程要处理），后续在调入，这个过程对于jvm是透明的。","categories":[],"tags":[{"name":"线程 Java","slug":"线程-Java","permalink":"http://kplxq.github.io/tags/线程-Java/"}],"keywords":[]},{"title":"10个小技巧让你的代码更Pythonic","slug":"10个小技巧让你的代码更Pythonic","date":"2018-04-27T12:17:24.000Z","updated":"2018-05-15T13:10:25.786Z","comments":true,"path":"2018/04/27/10个小技巧让你的代码更Pythonic/","link":"","permalink":"http://kplxq.github.io/2018/04/27/10个小技巧让你的代码更Pythonic/","excerpt":"Python 是一门兼具简单与功能强大的编程语言，拥有丰富的第三方库，在数据分析、AI、机器学习、Web 开发、运维、测试等多个领域都有不俗的表现。Python界中有一句非常有名的口号——人生苦短，我用Python。","text":"Python 是一门兼具简单与功能强大的编程语言，拥有丰富的第三方库，在数据分析、AI、机器学习、Web 开发、运维、测试等多个领域都有不俗的表现。Python界中有一句非常有名的口号——人生苦短，我用Python。 假如要交换变量a和b的值，你会怎么做？…是这样吗？ 12345a = 1b = 2temp = aa = bb = temp 隔壁的靖神觉得low爆了，竟然还用了多余的变量？其实有更Pythonic的做法 123a = 1b = 2a, b = b, a Pythonic是什么？所谓Pythonic简单说就是用Python的特有的语法去实现相应的功能，使代码更简洁、优雅、通俗易懂，更重要的是，看上去（diao）堡了。下面就教你如何使用正确的姿势。 如何让你的代码更Pythonic？1、遵循Python PEP8代码规范 这里只想说，驼峰式命名以经深入骨髓，不是说Python不支持这样的格式，只是使用带有PEP8插件的工具有助于你养成良好编码规范，更符合大部分用户的习惯，比如：123def helloWorld(): outputMessage = &quot;Hello World&quot; print outputMessage PEP8代码规范：123def hello_world(): output_message = &quot;Hello World&quot; print output_message 2、用全局变量file实时获取当前文件夹和文件名12345678# 获取当前文件路径print __file__ # 获取当前文件夹名称print os.path.basename(__file__) # 获取当前文件夹的目录print os.path.join(os.path.dirname(__file__)) # 获取当前文件夹的上一层目录print os.path.join(os.path.dirname(os.path.dirname(__file__))) 3、动态加入PYTHONPATH PYTHONPATH是python搜索模块路径集，有时自已写了一些模块执行时会报错no module named XXX,这就是python找不到此模块,此时我们可以动态的将目录加入到PYTHONPATH当中，这样即使分享代码给别人用，也能正常执行。123# 将当前文件夹实时加入到PYTHONPATH当中sys.path.append(os.getcwd())print sys.path 4、使用xrange代替range处理大文档 迭代器并不像list一样，直接存储进内存，想想处理一个1G文档的情况。12345type_iterator = xrange(10)type_list = range(10)print type(type_iterator) # 返回的是迭代器print type(type_list) # 返回的是list 5、使用列表推倒式获取文件 假如想获取当前目录下的文件，不考虑子文件夹的内容，通常可能这么做：12345file_list = list()for item in os.listdir(r&quot;/tmp&quot;): if os.path.isfile(item): file_list.append(item)print file_list 而用列表推倒式一行就搞定了：12file_list = [item for item in os.listdir(r&quot;/tmp&quot;) if os.path.isfile(item)]print file_list 6、使用enumerate获取对象索引和元素 为了同时获取一个可迭代对象的索引和元素，通常做法：123a_list = [1, 2, 3, 4]for i in xrange(len(a_list)): print i, a_list[i] 使用enumerate时：123a_list = [1, 2, 3, 4]for i, item in enumerate(a_list): print i, item 7、使用join拼接字符串 平时我们可能需要写入一个csv，标题是通过逗号分割，标题元素存在一个list当中。通常做法：12345678head_list = [&quot;username&quot;, &quot;password&quot;, &quot;email&quot;]head_str = &quot;&quot;for item in head_list: head_str += item + &quot;,&quot;print head_str使用join时：head_list = [&quot;username&quot;, &quot;password&quot;, &quot;email&quot;]print &quot;,&quot;.join(head_list) 8、多使用内置函数 尤其对列表进行一些计算时，如求和、最大值、最小值、排序等，以求合为例，通常是循环后累加：12345a_list = [1, 10, 5, 7, 9]total = 0for item in a_list: total += itemprint total 使用内置全局函数就简单多了：12345678from operator import mula_list = [1, 10, 5, 7, 9]print sum(a_list)print max(a_list)print min(a_list)print reduce(mul, a_list) # list中所有元素相乘print sorted(a_list) # 简单的从小到大排序 9、使用模板方式格式化字符串 通常做法：1print &quot;Life is Short , Please Use %s -- %s&quot; % (&quot;Python&quot;, &quot;Bruce Eckel&quot;) 使用format，位置参数时：1print &quot;Life is Short , Please Use &#123;0&#125; -- &#123;1&#125;&quot;.format(&quot;Python&quot;, &quot;Bruce Eckel&quot;) 前两种方式来看，%s和使用占位符都不是那么清晰，而通过模板的方式就非常明了了：1print &quot;Life is Short , Please Use &#123;language&#125; -- &#123;name&#125;&quot;.format(language=&quot;Python&quot;, name=&quot;Bruce Eckel&quot;) 10、用装饰器简化代码 装饰器并不算是Python特有的功能，其他语言也存在，使用得当可以大幅度减少重复性代码。假设我们想看某些方法到底运行了多少时间，总不能每一个方法都获取一个start_time、end_time，再相减得到运行时间吧。123456789import timedef time_cost(func): def wrapper(*args, **kwargs): start_time = time.time() func(*args, **kwargs) end_time = time.time() print &quot;TIME COST:&quot; + str(end_time - start_time) return wrapper 通过装饰器可以在任何一个函数上获取其执行的时间，而不会修改原函数执行的逻辑：123456@time_costdef test_decorator(): count = 0 for i in xrange(10000000): count += i print count 好了，通过几个简单的例子，有没有一种使用Python新姿势的感觉，后续我们会介绍更多Pythonic的技巧，大家拭目以待。 python彩蛋：python之禅，进入python窗口后输入import this12345678910111213141516171819202122232425262728293031323334353637383940&gt;&gt;&gt; import thisThe Zen of Python, by Tim PetersBeautiful is better than ugly.Explicit is better than implicit.Simple is better than complex.Complex is better than complicated.Flat is better than nested.Sparse is better than dense.Readability counts.Special cases aren&apos;t special enough to break the rules.Although practicality beats purity.Errors should never pass silently.Unless explicitly silenced.In the face of ambiguity, refuse the temptation to guess.There should be one-- and preferably only one --obvious way to do it.Although that way may not be obvious at first unless you&apos;re Dutch.Now is better than never.Although never is often better than *right* now.If the implementation is hard to explain, it&apos;s a bad idea.If the implementation is easy to explain, it may be a good idea.Namespaces are one honking great idea -- let&apos;s do more of those!翻译如下（摘自互联网）：优美胜于丑陋（python 以编写优美的代码为目标）明了胜于晦涩（优美的代码应该是明了的，命名规范，风格相似）简洁胜于复杂（优美的代码应当是简洁的，不要有复杂的内部实现）复杂胜于凌乱（如果复杂不可避免。那代码间也不能有难懂的关系，要保持接口简洁）扁平胜于嵌套（优美的代码应当是扁平的，不能有太多的嵌套）间隔胜于紧凑（优美的代码有适当的间隔，不要奢望一行代码解决问题）可读性很重要（优美的代码可读的）即便假借5特例的实用性之名，也不可违背这些规则（这些规则 至高无上）不要包容所有错误，除非你确定需要这样做（精确地捕获异常，不写 except：pass 风格的代码）当存在多种可能，不要尝试去猜测而是尽量找一种，最好是唯一一种明显的解决方案（如果不确定，就用穷举法）虽然这也不容易，因为你不是python之父（这里的dutch是指guido）做也许好过不做，但不假思索就动手还不如不做（动手之前要细思量）如果你无法向人描述你的方案，那肯定不是一个好方案；反之亦然（方案评测标准）命名空间是一种绝妙的理念，我们应当多加利用（倡导与号召）","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://kplxq.github.io/tags/Python/"}],"keywords":[]},{"title":"MBT-模型驱动测试的探索与实践（一）","slug":"MBT-模型驱动测试的探索与实践（一）","date":"2018-04-20T12:15:43.000Z","updated":"2018-05-15T13:09:24.454Z","comments":true,"path":"2018/04/20/MBT-模型驱动测试的探索与实践（一）/","link":"","permalink":"http://kplxq.github.io/2018/04/20/MBT-模型驱动测试的探索与实践（一）/","excerpt":"","text":"最近遇到了个这样的问题：chapter1 我们家产品汪宣布要盖一栋新型狗屋，于是开发汪哼哧哼哧开始了开发，而测试喵写了200+的用例来检测狗屋的质量；chapter2 产品汪又宣布要盖一栋复合型的狗屋，开发汪说好办，我们把之前的狗屋模型拿来改改就行，而测试喵又满怀激情的写了200+的用例；chapter3、chapter4 again again again测试喵突然有一天感觉腰也酸了，腿也疼了，干不动了，当测试在复制黏贴中沦为纯体力活，激情和价值都不复存在。 测试喵想：针对这些业务流程、规则类似，但可能在某些分支场景上存在差异的产品，用例似乎可以复用，不需要完全重新设计，但是到底哪些是一致的可以共用的，哪些是需要单独设计的呢？难道完全靠经验来筛选么？出了问题完全看命？改变工作方式的需求呼之欲出，模型驱动测试-MBT进入了测试喵的视线。 模型驱动测试MBT是什么今天第一篇开篇，我们先来分享一下模型驱动MBT的概念，之后我们会有篇续文分享我司在这方面的实践和探索。 模型驱动测试MBT顾名思义就是基于模型的测试，核心在于模型，模型是对被测系统的抽象。 模型驱动测试MBT并不是什么新鲜的概念，事实上IBM的Rational Rhapsody TestConductor Add On 和 Rational Quality Manager，微软的Spec Explorer都是基于模型驱动的测试工具，腾讯等大厂在这方面均有实践，只是暂时没有看到通用性很强的工具。 常见MBT建模种类我们通常建立的模型有以下几种：Mental Model（心智模型）：即是人们对于世界的理解方式是透过询问：这是什么？为什么这样？这样有什么目的呢？这个东西是如何运作？它会造成什么后果？将这些问题简化成下列的架构图，这里我们可以将其归纳为对产品的理解、设想和体验。 SUT Model：System Under Test Model 是心智模型（对产品的理解、设想和体验）的外化（以及与现有模型的整合），是一种图形化或形式化的类比模型。它涉及到不同的层次（如系统、组件和工作环境）、不同视角（如语境/上下文、组件与结构、功能、行为和用户体验）和不同关注点（如数据类型、因果关联、程序结构、任务控制、动作、事件和接口）等经过抽象、泛化和删减后，SUT模型只保留有助于实现特定测试目的的特征。SUT模型的实例化可能用到的技术包括Finite State Machine (FSM)，Message Sequence Chart (MSC)， Control FlowGraph (CFG)，Event Flow Diagram，MarkovChains和UML Testing Profile ，语法测试（SYNTAX TESTING），NLP（自然语言语义模型），此外还有从整体视角的HTSM和ACC等等。 TRM：Test-Ready Model 是对SUT模型的扩展和转化，以使模型达到可测试的标准；该模型也可独立使用，即给出相关信息，我们就可以设计或使用一套测试设计算法，用来产生可以运行的测试用例。它根据SUT模型特征和项目实际情况增加或凸显质量风险信息。必要时TRM需要创建新的模型，这是测试建模的主要难点之一，但也体现了我们价值所在。另外，它转化SUT模型以达到可测试标准，并增加“怎么测”的信息，同时为SUT模型修改重构提供反馈。TRM目前缺少成熟的工具和方法，是MBT的难点和研究方向；可见我们平常通过流程图来推演测试场景的过程本身也是建模的过程，并没有那么高大上对不对。 MBT建模途径建模的过程又有以下几种路径，不同的路径决定了不同的测试设计过程： 路径一（红色箭头）：从心智模型（Mental Model）直接得到测试用例（Ad-hoc Test Design，基于临时需求）；路径二（黄色箭头）: 从心智模型（Mental Model）得到TRM模型，再由TRM模型生成测试用例（传统测试设计）；路径三（蓝色箭头-&gt;紫色箭头）：从心智模型（Mental Model）到SUT模型，再由SUT模型生成测试用例（教科书式）；路径四（蓝色箭头）：从心智模型（Mental Model）到SUT模型，再由SUT模型到TRM模型，最终由TRM模型生成测试用例（MBT）。 实践举例说了这么多理论，我们来举个实际的例子（路径四）： 需求描述：投资中我们需要增加起投金额的判断假设项目剩余金额为X，投资金额Y输入校验： a.Y大于等于起投金额Z； b.X-Y需大于等于起投金额Z； a、b条件为且的关系 第一步，建立Mental Model（心智模型）： 需求：投资金额合法性判断1.投资金额&gt;=起投金额 且 项目剩余可投金额-投资金额&gt;=起投金额 投资成功2.投资金额&lt;起投金额 投资失败3.项目剩余可投金额-投资金额&lt;起投金额 投资失败 第二步，根据心智模型建立SUT模型： 第三步，根据SUT建立TRM模型：TRM通过SUT建立的模型，增加检查点等，将SUT转化成可执行的用例；1、投资金额&lt;起投金额2、投资金额=起投金额3、….. 第四步：将模型转换成用例模型建立好，我们还需要工具支持将我们的模型转化成用例（最好是可执行的用例）。 MBT测试过程 A、根据需求选择合适的模型来描述被测试对象（测试设计的核心）B、根据模型生成测试用例及期望结果（MBT工具的核心），如果能够直接生成可执行的用例最好；C、在被测系统上执行用例D、比较系统行为及输出和预期结果、反馈验证结果；虽然MBT工具使用的语言千差万别但是基本过程基本一致，我司的MBT测试工具也是这个思路。 MBT带来的好处1、模型建立的过程有助于我们从立体的角度重新认识我们的被测对象，同时也把我们对被测对象的理解通过模型化的方式表达出来，能够更高效的和开发团队中的其他人员沟通；2、如果模型建立适当，我们可以获得我们的基线业务模型、最大程度的减少我们测试设计上的重复工作量，聚焦于变化的部分，并将这种分析从单纯的靠经验转化成通过工具来实现；3、我们可以通过维护模型来保持我们的产品知识库、测试设计、测试数据、用例与系统当前实现的一致性，避免由于版本的快速迭代造成的文档和实际系统脱节的问题；4、为测试leader在测试范围评估过程中的识别风险模块提供现实的依据。 总结一句话，懒惰才是推动人类科学进步的动力，就像电梯是因为我们懒得爬楼才应运而生的，扫地机器人是因为我们懒得扫地才有市场的，现在我们也懒得反复造轮子写一堆几乎一样的用例，我们要引入新的测试方法，把我们的力气发挥在更有价值的地方。","categories":[],"tags":[{"name":"模型驱动测试 测试","slug":"模型驱动测试-测试","permalink":"http://kplxq.github.io/tags/模型驱动测试-测试/"}],"keywords":[]},{"title":"最接地气的区块链应用","slug":"最接地气的区块链应用","date":"2018-04-16T09:55:42.000Z","updated":"2018-04-16T10:00:06.744Z","comments":true,"path":"2018/04/16/最接地气的区块链应用/","link":"","permalink":"http://kplxq.github.io/2018/04/16/最接地气的区块链应用/","excerpt":"","text":"恕在下直言,网易星球是迄今为止最接地气区块链应用(暂无之一) ! 相较于当前市面上各类敛财大于实质的ICO、猫狗^注1^等区块链应用,网易星球秉持网易一贯的趣味、易用之研发原则,以独特视角诠释区块链应用,今天笔者将以尽量简短的篇幅带大家走进网易星球(本文章预计阅读时间5分钟) 关键概念相较于其他区块链应用,网易星球通过简化概念,弱化各类专业术语,以趣味性的游戏术语向用户暴露其应用中的关键因素。以期达到任意小白用户均可理解、上手的目的。相关的关键名词如下表所示 名词 官方解释 备注 星钻 黑钻是基于个人星球活动产生的奖励,可以用于星球上的消费与兑换等 持有的币值 原力 原力是用户获取黑钻的影响因子,原力越高,获得的黑钻越多 币值分配因子 核心流程基于网易星球应用中的关键概念,我们不难看出,其本质依旧是数字货币,通过”黑钻”的生产、分配、汇兑来进行串联,在下面的章节,笔者将基于”黑钻”的各个环节为大家讲解。 黑钻的生产根据网易星球的官方声明,其黑钻由平台自主生产,当前每日产量270000个,每2年减少一半。其生产机制与当前众多数字货币类区块链应用有较大差异,其优势在于: 降低用户参与门槛,黑钻的生产完全由平台自主进行,用户无需购买昂贵的挖矿设备,更无需自行构建复杂的挖矿环境,这对于非IT从业人员的用户来说无疑具备极大的诱惑力 极大降低能源耗损,相较于广受诟病的POW机制,平台自主生产黑钻,可有效规避此类高耗损行为 然而,平台自主生产也带来了一个致命的问题: 网易星球当前自主生产黑钻,从表面看其是典型的中心化应用,虽然其研发团队成员号称其使用的是联盟链,但是在其白皮书及源码未发布前,网易星球是伪链的质疑声不会停止。 黑钻的分配黑钻按固定产值生产,并通过一定的分配原则向用户进行分配,根据其官方声明,其分配规则如下: ​ 用户每日领取到的黑钻 = C该用户当前原力值/所有用户原力值之和 (C为每日黑钻总数)* 基于上述公式,在每日黑钻总数一定的前提下,用户每日可获取的黑钻总数与当前用户原力数值及其他用户所持有原力总和相关,这也就代表着,用户如果要最大化地获取黑钻,必须不断的获得原力,而当前网易星球获取途径有:邀请新用户注册、每日登陆、使用网易系产品(网易云音乐、网易课堂等)。 而基于网易星球在其”星球基地介绍”页中的描述(详见图”个人价值挖掘”),其后续会提供更多的原力获取方式,其基本方针应该还是先内后外,通过网易或者外部商户在网易星球内发布任务(任务包括但不限于:数据分享、应用下载及使用等),用户完成指定任务获取原力。而这也在其”星球基地建设规划”中得到证实。 黑钻的汇兑黑钻作为数字货币,其无法规避两个问题: 与法定货币的汇率 相较于其他数字货币通过各类交易所交易^注2^方式来确定其与法定货币的汇率,网易星球采用了通过确值商品竞拍模式来锚定与法币的汇率。而基于3月27日第一份竞拍成交订单来看,当前黑钻价格已达到了15元/个!!! 价值的兑换 当前在网易星球上,黑钻无法在用户间进行转账操作,暂只可通过竞拍模式竞拍网易系的一些确值商品,而随着参与用户越来越多,原力及黑钻获取难度越来越大,可以预见的是,黑钻价格将越来越高,而网易星球是否会引入外部商户商品加入,笔者个人认为不会,毕竟引入商品的实质并非促进指定商品的销量,而只是为黑钻的价格进行锚定而已。 基于上述章节的描述,相信大家已经对网易星球已经有了个大致的了解,易用性、趣味性是其特质,然而,是否网易星球也像三石老板外表一样憨厚朴实,人畜无害?非也,网易星球应用从一开始便是挂在网易金融板块之下,而在其内测之初,便有网上测评人士指出其通过诱导用户分享个人数据构建金融信用数据体系,然而这已非本文之范围了,更何况”中国人更开放,他们也愿意用隐私交换便捷性” 相关链接网易星球回应伪区块链与数据授权机制质疑：基于联盟链，共识未予公布 尴尬中隐藏真相？网易星球黑钻收购价暴跌90%至3元 标注解释注1:网易第一款区块链游戏名为《网易招财猫》 注2:截止笔者发文之日,已有部分数字货币交易平台对网易星球黑钻进行交易,当前价格约为3.8元,峰值价格28.88元(玩客网)","categories":[],"tags":[{"name":"区块链","slug":"区块链","permalink":"http://kplxq.github.io/tags/区块链/"}],"keywords":[]},{"title":"知识是一种概率","slug":"知识是一种概率","date":"2018-04-13T07:24:51.000Z","updated":"2018-04-13T07:32:00.712Z","comments":true,"path":"2018/04/13/知识是一种概率/","link":"","permalink":"http://kplxq.github.io/2018/04/13/知识是一种概率/","excerpt":"最近在看《那些让你更聪明的科学新概念》时，了解到“基本概率”的概念: 每当一个统计学家想要基于现有证据预测事件发生的概率时，有两个必须考虑的信息点：证据本身的可靠性，我们必须计算它的可靠程度；单纯以相对发生率来计算事件发生的可能性。第二种数据其实就是基本概率。 书中给出一个容易忽略基本概率的例子，我看了答案之后发现自己算错了。","text":"最近在看《那些让你更聪明的科学新概念》时，了解到“基本概率”的概念: 每当一个统计学家想要基于现有证据预测事件发生的概率时，有两个必须考虑的信息点：证据本身的可靠性，我们必须计算它的可靠程度；单纯以相对发生率来计算事件发生的可能性。第二种数据其实就是基本概率。 书中给出一个容易忽略基本概率的例子，我看了答案之后发现自己算错了。 假设你参加了某种罕见癌症的检查。在一般人群中，这种癌症的罹患概率是1%（基本概率），而广泛的实验证明，这项检查的准确率是79%。更精确地说，尽管检查不会漏诊这项癌症，但是有21%的可能会被误诊为这个癌症，也就是所谓的假阳性。如果你接受了检查，检查结果是阳性的，那么，真的患癌的可能性是多大呢？ 这个例子可以用贝叶斯公式计算出来，应该是4.6%[^1]，我错误之处在于遗漏了“不会漏诊”这个条件，也就是如果患癌，则一定能检测到。大多数人的第一印象是从检查的可信度近80%得出确实患癌的可能性就是80%左右，这是错的。因为他们只关注到了检查的可信度，却忽视了基本概率。 另一方面，证据本身的可靠性也很重要。比如前段时间在学术圈被火热讨论的新闻“美国政治学顶级学术期刊《政治分析》宣布禁用P值”。这里的P值用于描述无效假设成立的可能性，现在学术界的一个被广泛认可的标准，是 P 值要小于 0.05。如果 P &gt; 0.05 ，别人会认为你这个结果很可能纯属巧合，根本不值得认真对待；如果 P &lt; 0.05 ，人们就说这个结果是“ 显著的 ”。 事实上，这个标准没有科学依据，只是约定俗成的，是由英国的统计学家罗纳德·费希尔（Ronald Fisher）在几十年前提出的。他当时选择了0.05这个数值，意思是 P &lt; 0.05 的结果才“值得看”。他其实认为P &lt; 0.001 才是可以接受的结论。 但问题在于，做实验想要得到 P 值小于0.001的结果，需要找太多受试者，成本实在太高。于是大家退而求其次，都默认了 0.05，其实这个标准都是很难达到的。 过去几年，在经济学、心理学等领域的论文中，P值的分布，在0.05处有明显的凸起，唯一的解释，就是有很多论文故意把P值做到了恰好在0.05以内。[^2] 所以，学习知识不仅要知其然，还要知其所以然。在《那些让你更聪明的科学新概念》中，意大利理论物理学家卡尔罗·罗威利（Carlo Rovelli）提到 本质上，知识是一种概率，这是当代实用主义哲学强调的概念。 科学就是在不断质疑中发展，知识可以随时间慢慢演化，将来新的证据和新的论证方法可能会改变现有的知识，所以也可以说很多知识是不确定的。数学物理学家弗里曼·戴森认为科学就是反叛的产物，只有反叛的心态才是科学的正确态度。而尼采也曾经提出，根本不存在什么绝对的、客观的真理。所以我们不能盲目地听从专家的意见，而应该亲自去验证知识的来源是否可靠，最好能像数学一样，一步一步地严格推理。 我们需要保持质疑精神，学会在冲突信息中随时获得新知，以应对快速变化的世界。 [1] 设A事件为患癌，B事件为检查呈阳性。P(A|B)=P(A)/P(B)P(B|A)=0.01/(10.01+0.990.21)1=0.045893[2] P&lt;0.05：科学家的隐藏动机.万维钢.2018.图中三个研究的出处可以在这里找到：http://datacolada.org/41","categories":[],"tags":[{"name":"基本概率","slug":"基本概率","permalink":"http://kplxq.github.io/tags/基本概率/"}],"keywords":[]},{"title":"代理模式及实现探究","slug":"代理模式及实现探究","date":"2018-04-08T07:14:03.000Z","updated":"2018-04-10T01:24:51.368Z","comments":true,"path":"2018/04/08/代理模式及实现探究/","link":"","permalink":"http://kplxq.github.io/2018/04/08/代理模式及实现探究/","excerpt":"前言使用过Spring并经常Debug或者熟悉Spring实现原理的开发应该知道动态代理是Spring框架实现的一大重要技术工具。 先看下图： 使用Spring依赖注入的bean通常都能看到CGLIB的标识。而我们自定义的类对象查看到对象信息基本和类定义无差。 究其原由是因为Spring依赖注入的bean并非原始的类对象，而是使用CGLIB的代理对象。 借由此本文旨在对代理模式及实现方式一探究竟。","text":"前言使用过Spring并经常Debug或者熟悉Spring实现原理的开发应该知道动态代理是Spring框架实现的一大重要技术工具。 先看下图： 使用Spring依赖注入的bean通常都能看到CGLIB的标识。而我们自定义的类对象查看到对象信息基本和类定义无差。 究其原由是因为Spring依赖注入的bean并非原始的类对象，而是使用CGLIB的代理对象。 借由此本文旨在对代理模式及实现方式一探究竟。 代理模式介绍概述因为某个对象消耗太多资源,而且你的代码并不是每个逻辑路径都需要此对象, 你曾有过延迟创建对象的想法吗? 你有想过限制访问某个对象,也就是说,提供一组方法给普通用户,特别方法给管理员用户? 以上两种需求都非常类似，并且都需要解决一个更大的问题:如何提供一致的接口给某个对象让它可以改变其内部功能,或者是从来不存在的功能? 可以通过引入一个新的对象，来实现对真实对象的操作或者将新的对象作为真实对象的一个替身。即代理对象。它可以在客户端和目标对象之间起到中介的作用，并且可以通过代理对象去掉客户不能看到的内容和服务或者添加客户需要的额外服务。 用书面术语来描述： 代理模式是常用的java设计模式。 它的特征是代理类与委托类有同样的接口。 代理类主要负责为委托类预处理消息、过滤消息、把消息转发给委托类，以及事后处理消息等。 代理类与委托类之间通常会存在关联关系，一个代理类的对象与一个委托类的对象关联，代理类的对象本身并不真正实现服务，而是通过调用委托类的对象的相关方法，来提供特定的服务。 分类按照使用场景： 远程代理（Remote Proxy）： 为一个位于不同的地址空间的对象提供一个本地的代理对象。这个不同的地址空间可以是在同一台主机中，也可是在另一台主机中，远程代理又叫做大使(Ambassador) 虚拟代理（Virtual Proxy）： 根据需要创建开销很大的对象。如果需要创建一个资源消耗较大的对象，先创建一个消耗相对较小的对象来表示，真实对象只在需要时才会被真正创建。 保护代理（Protection Proxy）： 控制对原始对象的访问。保护代理用于对象应该有不同的访问权限的时候。 智能指引（Smart Reference）： 取代了简单的指针，它在访问对象时执行一些附加操作。 Copy-on-Write代理： 它是虚拟代理的一种，把复制（克隆）操作延迟到只有在客户端真正需要时才执行。一般来说，对象的深克隆是一个开销较大的操作，Copy-on-Write代理可以让这个操作延迟，只有对象被用到的时候才被克隆。 按照实现方式： 静态代理： 在程序运行前就已经存在代理类的字节码文件。代理类和委托类的关系在运行前就确定了 动态代理： 动态代理类的源码是在程序运行期由JVM根据反射机制动态生成的。代理类和委托类的关系是在程序运行时确定的 UML图示 跟着Demo深入探究接下来我们自己动手用demo来实践一下代理模式的实现，并比较不同实现方式的区别。 公共接口及类定义： 12345678public interface TestService &#123; /** 打印入参字符串 */ void saySomething(String str); /** 返回自增+1 */ int countInt(int num);&#125; 12345678910public class TestServiceImpl implements TestService &#123; @Override public void saySomething(String str) &#123; System.out.println(str); &#125; public int countInt(int num) &#123; return (num++); &#125;&#125; 静态代理实现demo我们先看静态代理如何完成上述接口实现的代理： 123456789101112131415161718192021222324252627282930313233public class ProxySubject implements TestService &#123; // 代理类持有一个委托类的对象引用 private TestService testService; public ProxySubject(TestService testService) &#123; this.testService = testService; &#125; /** * 将请求分派给委托类执行，记录任务执行前后的时间，时间差即为任务的处理时间 * * @param taskName */ @Override public void saySomething(String something) &#123; Date startDate = new Date(); System.out.println(\"开始调用目标类时时间点：\" + startDate); // 将请求分派给委托类处理 testService.saySomething(something); Date endDate = new Date(); System.out.println(\"结束调用目标类时时间点：\" + endDate); &#125; @Override public int countInt(int num) &#123; return testService.countInt(num); &#125;&#125; 1234567public class Client &#123; public static void main(String[] args) &#123; TestService proxy = new ProxySubject(new TestServiceImpl()); proxy.saySomething(\"Hello buddy\"); &#125;&#125; 下面是上述代理执行结果： 开始调用目标类时时间点：Mon Oct 17 11:01:04 CST 2016 Hello buddy 结束调用目标类时时间点：Mon Oct 17 11:01:04 CST 2016 静态代理实现总结： 上述例子，静态代理类做的事情即是 在真实调用目标接口实现时打印接口调用的请求时间 调用目标接口 目标接口调用结束后打印请求完成时间 我们发现： 使用了静态代理。我们不需要入侵真实的目标类即可在目标对象调用时封装一套额外的逻辑。当然这是代理模式的有点，不局限于静态代理 为了实现接口的代理，我们必须要定义一个代理类实现同一个接口，在实现中显示的调用目标接口来完成代理的实现 基于这点，这也反应了静态代理实现的一大缺点： 一个代理对象服务于同一类的对象。业务类的所有方法都需要进行代理；业务类每新增一个接口，所有的代理类也要实现这个接口，增加代码维护的复杂度 JDK动态代理Demo上述静态代理的缺陷，而动态代理的特性正好可以解决。 而动态代理也有很多种实现技术手段。这一节讲讲java提供的原生动态代理： 123456789101112131415161718192021/** * JDK动态代理类 */public class ProxyHandler &#123; public static Object getPoxyObject(final Object c) &#123; return Proxy.newProxyInstance(c.getClass().getClassLoader(), c.getClass().getInterfaces(), // JDK实现动态代理，但JDK实现必须需要接口 new InvocationHandler() &#123; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object reObj = null; reObj = method.invoke(c, args); if (method.getName().equals(\"saySomething\")) &#123; System.out.println(\"at [\" + Calendar.getInstance().get(Calendar.HOUR) + \":\" + Calendar.getInstance().get(Calendar.MINUTE) + \":\" + Calendar.getInstance().get(Calendar.SECOND) + \"]\\n\"); &#125; return reObj; &#125; &#125;); &#125;&#125; 123456789101112131415161718/** * 测试客户端 */public class JDKServiceMain &#123; public static void main(String[] args) &#123; TestService service = new TestServiceImpl(); TestService poxyService = (TestService) ProxyHandler.getPoxyObject(service); System.out.println(\"\\n\\nexcute info:\\n\"); poxyService.saySomething(\"Manager Zhou: Hello, GentleSong.\"); poxyService.saySomething(\"Manager Zhou: you are KXF's dream guy.\"); poxyService.saySomething(\"Manager Zhou: Are you willing to sacrifice for the happniess of KXF's buddy?\"); poxyService.saySomething(\"GentleSong: Yes, I am.\"); &#125;&#125; 下面是上述代理执行结果： excute info: Manager Zhou: Hello, GentleSong. at [11:35:10] Manager Zhou: you are KXF&apos;s dream guy. at [11:35:10] Manager Zhou: Are you willing to sacrifice for the happniess of KXF&apos;s buddy? at [11:35:10] GentleSong: Yes, I am. at [11:35:10] JDK动态代理实现总结： 上述例子，动态代理类做的事情即是 调用目标接口 调用结束时打印请求调用完成时间 我们发现： 从JDK的实现方式看，它可以实现一类接口的的代理。 我们不需要每新增一个接口即新增一个代理类实现 我们不需要接口定义新增或删减时同时要修改代理类 但是，JDK的动态代理依靠接口实现，如果有些类并没有实现接口，则不能使用JDK代理。 CGLIB动态代理DemoJDK的动态代理是基于接口实现的。那没有接口定义的类实现如何代理嘞？ CGLIB能够解决这个问题。 12345678910111213141516171819202122232425262728/** * CGLIB动态代理类 */public class ProxyHandler &#123; public static Object getPoxyObject(Object c) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(c.getClass()); enhancer.setCallback(new MethodInterceptor() &#123; public Object intercept(Object arg0, Method arg1, Object[] arg2, MethodProxy proxy) throws Throwable &#123; proxy.invokeSuper(arg0, arg2); if (arg1.getName().equals(\"saySomething\")) &#123; System.out.println(\"at [\" + Calendar.getInstance().get(Calendar.HOUR) + \":\" + Calendar.getInstance().get(Calendar.MINUTE) + \":\" + Calendar.getInstance().get(Calendar.SECOND) + \"]\\n\"); &#125; return null; &#125; &#125;); return enhancer.create(); &#125;&#125; 12345678910111213141516171819202122232425public class ProxyHandler &#123; public static Object getPoxyObject(Object c) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(c.getClass()); enhancer.setCallback(new MethodInterceptor() &#123; public Object intercept(Object arg0, Method arg1, Object[] arg2, MethodProxy proxy) throws Throwable &#123; proxy.invokeSuper(arg0, arg2); if (arg1.getName().equals(\"saySomething\")) &#123; System.out.println(\"at [\" + Calendar.getInstance().get(Calendar.HOUR) + \":\" + Calendar.getInstance().get(Calendar.MINUTE) + \":\" + Calendar.getInstance().get(Calendar.SECOND) + \"]\\n\"); &#125; return null; &#125; &#125;); return enhancer.create(); &#125;&#125; CGLIB动态代理实现总结： 上述例子，动态代理类做的事情即是 调用目标接口 调用结束时打印请求调用完成时间 CGLIB实现原理： CGLIB是针对类来实现代理的，他的原理是对指定的目标类生成一个子类，并覆盖其中方法实现增强（当然这运用到了java的又一特性：修改字节码）。由于使用的是继承的方式，如果类或者方法被声明为final，将无法使用CGLIB的动态代理。 几种代理实现方式的比较上述几种代理实现方式实践之后，大家是否就粗暴的认为CGLIB动态代理的方式是最优项嘞？毕竟它解决了静态代理和JDK动态代理的缺陷。 这里我们不要这么快给出判断。同样实践得真知，下面我们用demo来测试一下几种代理实现方式的性能： 测试分为以下几个维度： 在单例模式（仅创建一次代理类）下分别执行100万、500万次静态代理、JDK动态代理、CGLIB动态代理 在多例模式（每次调用新创建代理类）下分别执行100万、500万次静态代理、JDK动态代理、CGLIB动态代理 细化测试代理对象创建、代理类执行的时耗测试 PS:在单例测试和多例测试请勿按比例比较，毕竟多例测试中不断地创建及销毁对象也是时间花销。但是不同实现方式的多例测试还是有参考性的。 具体的代码示例已上传至GitHub，链接地址为：https://github.com/huangnanxi/proxyDemo PS： demo的例子中：cglib采用的是对类的代理的调用方式（setSuperClass,invokerSuper） 基于这样的前提：比较，我们得出以下结论： 在单例模式下。JDK动态代理与CGLIB动态代理性能相差不大 在多例模式下。JDK动态代理的性能远大于CGLIB动态代理 JDK1.7的JDK动态代理性能较于JDK1.6有明显的提升 深入背后的原因是： JDK在创建代理（生成字节码时）效率远大于CGLIB JDK动态代理执行与CGLIB动态代理（类代理）执行的效率相差无几 若cglib采用的是对接口的代理的调用方式（setInterface,invoker)（大家可以修改一些demo验证一下） 结论也同cglib类代理","categories":[],"tags":[{"name":"java 动态代理","slug":"java-动态代理","permalink":"http://kplxq.github.io/tags/java-动态代理/"}],"keywords":[]},{"title":"SpringBoot2.0之WebFlux开发实战（含源码）","slug":"SpringBoot2-0之WebFlux开发实战（含源码）","date":"2018-04-03T06:06:55.000Z","updated":"2018-04-04T06:25:36.357Z","comments":true,"path":"2018/04/03/SpringBoot2-0之WebFlux开发实战（含源码）/","link":"","permalink":"http://kplxq.github.io/2018/04/03/SpringBoot2-0之WebFlux开发实战（含源码）/","excerpt":"15分钟和你一起聊一聊2.2万星超热门开源项目Spring Boot 2.0之WebFlux开发，从技术介绍、开发教程、集成案例演示到示例源代码，一网打尽。","text":"15分钟和你一起聊一聊2.2万星超热门开源项目Spring Boot 2.0之WebFlux开发，从技术介绍、开发教程、集成案例演示到示例源代码，一网打尽。 背景知识Spring Boot2.0北京时间3月1日，Spring Boot 2.0正式发布Release版本。作为Spring生态中重要的开源项目,Spring Boot旨在帮助开发者更容易的创建基于Spring的应用程序和服务。经历了4年的发展，Spring Boot已经拥有了22000多star，16000次Commits，贡献者超过400多名的超热门开源项目。其中刚发布的2.0版本是自2014年4月1日发布的1.0版本以来第一次重大修订，也是首个提供对Spring Framework 5.0支持的GA稳定版本，2.0带来了很多新的特性✓基于Spring 5构建的Spring Boot 2.0，通过使用Spring WebFlux 提供了响应式Web编程支持；✓基于Java 8（最低标准），支持Java 9；✓支持HTTP/2;Spring Boot的Web容器选择中Tomcat、Undertow和Jetty均已支持HTTP/2。HTTP/2较HTTP/1.1在性能上有显著提升，页面加载时间降低了50%，详见Java 9和Spring Boot2.0纷纷宣布支持的HTTP/2到底是什么。其余新特性不在此赘述，详见Spring Boot 2.0 Release Notes。 Spring WebFluxSpring WebFlux是一个非阻塞的函数式Reactive Web框架，可以用来构建异步的、非阻塞的、事件驱动的服务，在伸缩性方面表现非常好。名称中的Flux来源于Reactor中的类Flux。 众所周知Spring MVC是同步阻塞的IO模型，资源浪费相对比较严重，当我们在处理一个耗时的任务时，例如上传一个较大的文件时，服务器的线程一直在等待接收文件，这期间什么也做不了，等到文件接收完毕可能又要写入磁盘，写入的过程线程又只能在那等待，非常浪费资源。而Spring WebFlux是这样做的，线程发现文件还没接收好，先去做其他事情，当文件接收完毕后通知该线程来处理，后续写入磁盘完毕后再通知该线程来处理，通过异步非阻塞机制节省了系统资源，极大的提高了系统的并发量。因此对于微服务下的IO密集型的Service来说，WebFlux是一个不错的选择。 左边是传统基于Servlet的Spring Web MVC框架，右边是Spring Framework 5.0引入的基于Reactive Streams的Spring WebFlux框架，从上到下依次是Router Functions、WebFlux和Reactive Streams三个新的组件。● Router Functions：对应@Controller、@RequestMapping等标准的Spring MVC注解，提供了一套函数式编程的API，用于创建Router、Handler和Filter。● WebFlux：核心组件，用于协调上下游各个组件提供响应式编程的支持。● Reactive Streams：一种支持Backpressure的异步数据流处理标准，主流实现有RxJava和Reactor，Spring、WebFlux默认集成的是Reactor。Backpressure是一种反馈机制，当数据的发布速度超过处理速度时，消费者需要决定缓存还是丢弃，在响应式编程中，决定权交回给了发布者，消费者只需根据自身处理能力向发布者请求相应数量的数据。●Web容器：Spring WebFlux既支持Tomcat、Jetty等传统容器（前提是支持Servlet3.1+）,又支持Netty、Undertow等异步容器。只能运行在Servlet 3.1+容器上，是因为3.1规范支持异步处理，该功能主要针对业务处理较耗时的情况，可以减少服务器资源占用，提高并发处理速度。 Spring WebFlux实战WebFlux工程创建本文默认开发环境是JDK8，开发工具是IntelliJ IDEA。下面我们基于Spring Boot 2.0创建一个WebFlux工程，操作步骤非常简单：1)点击Create New Project,创建一个新的项目； 2)选择Spring Initializr,并配置JDK版本为1.8，Initializr Service UR按默认配置为https://start.spring.io; 3)在metadata页配置工程包名等信息，其中type项目构建方式选择默认值，使用maven进行项目构建； 4)在Dependencies页，我们先将Spring Boot版本设置为2.0.0，可以看到下方有很多选项可以选择，每个选项代表一个组件，这里选择“Web”-&gt;“Reactive Web”组件。可以看到下方提示，Reactive Web development with Netty and Spring WebFlux，即通过Reactive Web构建一个WebFlux应用服务； 5)最后配置下工程名称和项目路径，即完成了Spring WebFlux应用的创建。 默认的demo工程pom中只包括webflux和reactor等组件依赖。 Hello World应用开发下面通过编写Handler和Router Functions来实现hello world程序。1)编写Hello world handler，该类相当于Spring Web中的Service bean； 2)将Hello world handler注册到路由上，类似于Spring Web中的Controller类的创建。除了新的Router Functions接口，Spring WebFlux同时支持使用老的Spring MVC注解声明Reactive Controller。 3)接着运行DemoApplication的main方法，即完成了服务启动，这里默认采用了Netty作为reactor的底层容器启动。 最后访问http://127.0.0.1:8080/hello，返回hello world即表示服务启动和访问成功。 注册登录应用开发需要注意目前支持reactive编程的数据库只有MongoDB，Redis，Cassandra，Couchbase，而JDBC与JPA的事务是基于阻塞IO模型的，并不是自然支持reactive编程风格，需要等待Spring Data Reactive升级IO模型才能支持相关数据库事务的使用。 这里以Redis作为数据库，实现一个简单的用户注册登录功能。1)首先配置Spring Data Reactive Redis，默认指向本地6379端口的Redis； 2)编写用户注册登录handler,主要通过RedisConnection进行数据入库和查询操作，并将业务处理结果以Json格式进行返回。 Uri中的参数可以通过ServerRequest.bodyToMono来获取。返回的类型Mono可以通过ServerResponse来创建，主要包括以下几步：a) 状态码方法，可以使用现成的也可以自定义，例如成功的状态码： b) ContentType(MediaType var1)返回的内容类型是MediaType类型；c) 最后是返回的内容： 一般常用body()来放入返回的内容，如使用BodyInserters的构建方法fromObject()。3)添加注册登录路由,将url路由给具体的handler来进行处理; 路由关系创建主要通过RouterFunctions来创建route, 其中RequestPredicate和HandlerFunction都是函数式接口，HandlerFunction接受一个ServerResponse的子类返回Mono,可以把这个对象认为是实际处理逻辑的部分。 下面我们来验证下程序的运行情况：1)首先运行服务，然后通过postman发送用户注册请求；2)发送注册请求后，除了前台返回{“message”:”successful”},同时可以查看Redis中保存的注册信息； 3)接着用刚注册的信息发起登录请求，可以看出返回结果为登录成功。 至此即完成了一个简单的基于Spring WebFlux和Redis的用户注册登录功能开发。 分析总结通过以上介绍，可以看出基于SpringBoot进行WebFlux开发即简单又高效。下面对WebFlux中几个关键语法点进行介绍：首先简单说下Reactor的两个关键概念，Mono和Flux是Reactor中的流数据类型，Mono是一个用来发送0或者单值数据的发布器，Flux可以用来发送0到 N 个值。它们表示在订阅这些发布服务时发送的数值流。如下图中getUserById()返回一个Mono表示其在数据可用的情况下发送0个或者单个用户，getUsers()返回一个用户列表的Flux实例，表示其发送0到多个用户数据。 上文提到的handler处理类相当于服务bean,一般用来编写业务功能，其中返回的ServerResponse类似Spring Web中的ResponseEntity用来封装响应数据，包括状态码、HTTP头等信息，它包含了ok(),notFound()等方法，用来创建不同类型的响应信息。如上图的UserRepository.getUserById()返回一个Mono，而ServerResponse.ok().body(Mono.just(user), User.class) 将这个Mono转成Mono,这代表在ServerResponse可用时候发送响应的流。ServerResponse.notFound().build()返回一个Mono对象，当给定的pathVariable中找不到对应用户信息时返回404的服务器响应信息。 Spring WebFlux除了对响应式http的支持外，还包括服务端推送事件（Server Sent Events,SSE）、WebSocket客户端和服务端的支持。其中服务端推送事件允许服务器不断地推送数据到客户端，它的实现非常简单，只需要返回对象类型配置成Flux,就会自动按照SSE规范要求的格式发送响应。在命令式的编程风格中，线程的执行会被堵塞，直到接收到数据。这使得数据在实际返回之前线程必须进行等待。而在Reactive编程中，我们定义了一个流，用来发送数据以及数据返回时所执行的操作。使用这种方法线程不会被堵塞的。当数据返回时框架会选择一个可用的线程进行下一步处理，这就体现出异步非阻塞模式的优势所在。因此响应式编程能带来更快处理速度，更高硬件利用率的未来选择。 参考资料 https://docs.spring.io/spring/docs/current/spring-framework-reference/web-reactive.html#webflux-fn-handler-functions https://coyee.com/article/12086-spring-5-reactive-web https://zhuanlan.zhihu.com/p/30813274 https://github.com/hantsy/spring-reactive-sample#spring-data-redis https://www.ibm.com/developerworks/cn/java/spring5-webflux-reactive/index.html 作者介绍刘海东，供职于开鑫金服，任移动端架构师、销售平台技术专家，主要方向是Java Web、Android、shell和C++，精力充沛，爱好广泛。","categories":[],"tags":[],"keywords":[]},{"title":"见女神的极简之途——BFS算法","slug":"见女神的极简之途——BFS算法","date":"2018-03-16T05:55:27.000Z","updated":"2018-04-04T06:28:44.845Z","comments":true,"path":"2018/03/16/见女神的极简之途——BFS算法/","link":"","permalink":"http://kplxq.github.io/2018/03/16/见女神的极简之途——BFS算法/","excerpt":"三月，空气中弥漫着恋爱的味道，程序员哧溜君忽然接到女神电话要去见丈母娘了（喜闻乐见），女神一再叮嘱头一次得好好地准备准备，而且不能迟到： 到【艾欧尼亚】做头发； 到【德玛西亚】买给岳父的保养品； 到【扭曲丛林】买给丈母娘的护肤品； 到【皮城警备】买给准小侄子的玩具； 最后才能到【女神家】……每个地方的距离都很远，如何合理地规划路线？这相似的场景，程序员哧溜君不觉笑了起来……","text":"三月，空气中弥漫着恋爱的味道，程序员哧溜君忽然接到女神电话要去见丈母娘了（喜闻乐见），女神一再叮嘱头一次得好好地准备准备，而且不能迟到： 到【艾欧尼亚】做头发； 到【德玛西亚】买给岳父的保养品； 到【扭曲丛林】买给丈母娘的护肤品； 到【皮城警备】买给准小侄子的玩具； 最后才能到【女神家】……每个地方的距离都很远，如何合理地规划路线？这相似的场景，程序员哧溜君不觉笑了起来…… BFS是什么BFS全称为Breadth-First-Search，即⼴度优先遍历算法，是经典的连通图遍历算法。简单的说， BFS是从根结点开始，沿着树的宽度遍历树的节点。如果所有节点均被访问则算法结束。 换成算法语来描述的话就是： 首先将根节点放到待遍历的队列Q中； 循环从Q中获取待遍历的节点N； 获取节点N通过条边可以到达的所有节点集合L； 循环遍历节点集合 L，判断L的元素是否存在于已遍历的节点集合C中； 已存在集合 C中，则忽略； 不存在集合 C中，则将此节点加⼊到队列 Q中； 循环遍历Q直到Q为空则遍历结束。（没看懂没关系，后面有详细讲解呢，哧溜~） BFS用来做什么BFS作为图遍历的基础算法，它的算法思想被很多高级算法借鉴和使用。 Dijkstra单源最短路径算法和Prim最小生成树算法都采用了和宽度优先搜索类似的思想。 我们可以对哧溜君见丈母娘之路进行建模，A点为哧溜君所在之处，B点为目的地，找到A与B之间的最短路径。 针对上述问题有很多算法方案可以解决，其中BFS算法是较简单和直接的方案。以A节点为根节点通过BFS遍历其他节点，直到遍历到B节点为止，这种理想情况下说起来还是比较简单的。 但是不要认为这样就可以完美解决这个问题，完美的解决方案还需要考虑以下因素： 哧溜君在行走的过程中，如何避免遇到前女友； 在那么多的目的地之中寻找最先去的目的地； 在哧溜君去往目的地的路上，判断是否需要翻墙、踩水坑、闯红灯； BFS怎么做以上都是理论知识，读起来可能比较枯燥。我们举个例子来进一步说明BFS算法。给定一个图，从图中一个节点出发，通过BFS遍历所有能遍历的节点。 图的初始状态如下： 初始状态，从顶点 1开始，队列 ={1}； 访问1的邻接顶点， 1出队变⿊， 2,3⼊队，队列 ={2,3,}; 访问2的邻接结点， 2出队，4⼊队，队列 ={3,4}; 访问4的邻接结点， 4出队，队列 ={空}; 经过上述遍历过程，我们已经通过BFS算法找到所有节点1可以到达的节点，进一步可以分析出节点1到每个节点的最短距离，哈哈哈哈哈…… 程序员哧溜君经过一番计算后， 终于……终于……从梦中醒来了。程序员怎么可能有女朋友啊，而且还是女神！还是去程序里继续new对象吧。","categories":[],"tags":[{"name":"算法 BFS","slug":"算法-BFS","permalink":"http://kplxq.github.io/tags/算法-BFS/"}],"keywords":[]},{"title":"别再低效学习了，快构建自己的知识体系吧！","slug":"别再低效学习了，快构建自己的知识体系吧！","date":"2018-03-09T05:42:47.000Z","updated":"2018-04-04T05:52:20.803Z","comments":true,"path":"2018/03/09/别再低效学习了，快构建自己的知识体系吧！/","link":"","permalink":"http://kplxq.github.io/2018/03/09/别再低效学习了，快构建自己的知识体系吧！/","excerpt":"我们每天都在主动或被动地接受各类文章的轰炸，点赞、转发、收藏、保存……然后呢，我们得到了什么？在工作中我们遇到了各种各样的问题，然后主动地去搜索资料并最终成功解决了，然后呢，我们学到了什么？在面对层出不穷的新技术、新知识点时，我们很努力很努力地去学习，却始终有种力不从心的感觉，甚至认为自己的学习能力下降，这是因为什么？","text":"我们每天都在主动或被动地接受各类文章的轰炸，点赞、转发、收藏、保存……然后呢，我们得到了什么？在工作中我们遇到了各种各样的问题，然后主动地去搜索资料并最终成功解决了，然后呢，我们学到了什么？在面对层出不穷的新技术、新知识点时，我们很努力很努力地去学习，却始终有种力不从心的感觉，甚至认为自己的学习能力下降，这是因为什么？ 一、什么是知识体系？知识体系，其实大家应该不算陌生了，通俗点的解释就是把一些零碎的、分散的、相对独立的知识概念或观点加以整合，使之形成具有一定联系的知识系统。这种系统就像是一棵树，每片叶子都是独立的，但树干把它们联系在一起，形成了体系，最典型的例子就是我们学生时代做的那些教辅资料，它们的每章末都会有自己的章末小结，对前一章的知识点进行整合，即是最常见的知识体系的构建了。 二、低效的知识学习方式构建知识体系的原理大家都懂，但真正实施起来又谈何容易，世界上的各类知识成千上万，又赶上互联网数据大爆炸的时代，知识早已不再是以“点状”或者“树状”的形态呈现，而是变成了一张密密麻麻纵横交错的“网”，以个人的能力将它们内化成知识体系几乎成了一种“吃力不讨好”的行为。 早些年在腾讯cdc博客曾流传过这样一张图（原作者： hsiang），用于描述我们日常生活中最常见的低效的知识管理现象。 知识收集为中心的学习观念每天热衷于各类知识的收集，比如在知乎上看到好的回答就想点收藏，但收藏之后又不会多看一眼，潜意识里不愿花较多的时间在知识的消化上，又偏偏是收集成瘾，最终导致采集的知识总量越来越大，逐渐产生一种时间不够用的错觉，最终在知识的海洋里迷失自我，整日忙忙碌碌、碌碌无为。 囫囵吞枣，不求甚解的学习观念“我懂了”与“我会用”其实是两个概念，很多时候我们都会高估自己实际运用的能力，做什么事都喜欢求大求全，自以为积累的各门类的知识越完整越好、越“成套”越好……殊不知这样建立起来的知识体系往往看似高大全，实则都是“纸老虎”，经不起实战的考验。 应当以自己的应用场景去架构自己的知识世界所谓的“应用场景”，其实理解起来很简单，就是我们掌握知识也好，建立体系也罢，其根本目的还是为了学以致用，你完全可以依据自己的专业或者工作建立一套高实用性的知识体系，让你可以随时能从中提取信息，并且运用到自己的生活和学习中。 三、建立知识体系的一般办法 明确知识体系的主题和用途简而言之，在构建自己的知识体系之前，你务必要明确自己体系的主题和目标，提醒自己“我的知识体系是什么，它的用途是什么，我又为什么要建立它？ 通过下图的金字塔，我会发现我自己现在每天正在学习和坚持的是在哪个区间里面： 总之一句话，学那些让自己变得更加专业的知识，让自己的专业知识金字塔变得更高。 找到获取知识的途径 书本： 针对某一领域，进行快速阅读和主题阅读，可以快速掌握某一领域的基础知识，形成最基础的知识框架，另外每本书一般都会有参考资料，这些也是很好的知识来源，只要你细心，完全可以按图索骥，从中找到适合自己建立“体系”的内容。 课堂： 这里的课堂既包括现实中的上课，也包括网络视频课程，像网易云课堂、中国MOOC等网站都是你获取知识的有效途径。 网络： 互联网时代，网络已然成为我们生活的必须品，只有你好好利用网络，其实能找到适合你“知识体系”的内容，这是你主动学习的一大“法宝”。 随时留心生活的点滴： 永远不要把知识累积全部放到主动学习上，有的时候生活当中的一本随手翻翻的书、一页无意中浏览到的网页、一次好友亲人之间的聊天，如果你足够敏感，都可以成为你知识体系中重要知识的来源。 知识的整理与分类知识的淘汰更新非常重要，我们是知识工作者，不是历史学家，很多信息甚至在我们收集到手之后就已经过期了。所以果断抛弃我们当下用不到的知识，用断舍离的方式来提醒我不要囤积“能用”但是当下对我没用的知识垃圾。 有了前面所说的主题和途径，你就可以按照逻辑和层次，分出尽量详细的项目类别了，这期间你既可以用手写笔记的形式整理，也可以利用网络管理资源，像有道云笔记、印象笔记、为知笔记、evernote等笔记类软件进行整合，专门建立一些“笔记本”，用你的“知识项目”来命名，然后把你的知识点（文字、图片、其他媒体）按照名称，作为笔记归纳进你的知识体系之中。 知识的输出与运用教是最好的学习，实现90%的知识转化，分享是最好的方式。 总结1、建立自己的知识体系，最重要的是思维上的转变，纠正“以知识的收集为中心”的学习观念以及“囫囵吞枣、不求甚解”的学习方法，改变低效的个人知识管理模式，应当以自己的应用场景去架构自己的知识世界。 2、纠正了思维上误区，还要掌握构建知识体系的一般方法。你需要在体系建立之前明确它的主题和用途，不在无关的内容上浪费自己宝贵的时间，之后你要学会利用身边的各种资源获取知识，随时留心生活中的点点滴滴，一举一动、一草一木都可以是获取知识的来源。 3、有了来源，还要学会知识的整理和分类，可以按照逻辑和层次，分出尽量详细的项目类别，期间充分利用像有道云笔记这种网络笔记类软件，分门别类，不断补充。 4、知识的输出和运用是我们建立知识体系的最终目的，可以用“费曼技巧”等方式检验自己的学习成果， 只有读进去能表达出来，才算是真正的吸收，从而获得难以替代的成就感。 5、有了体系也千万不要墨守成规， 因为你会不断地遇到新的信息，必然会对原来的结构造成影响，这时候你要及时把握学术动态，更新知识体系，一定不能让自己的大脑僵化，而是要打破原来的体系不断对新的事物进行接受。 开普勒鑫球-贾克斯收集、整理。","categories":[],"tags":[{"name":"学习方法","slug":"学习方法","permalink":"http://kplxq.github.io/tags/学习方法/"}],"keywords":[]},{"title":"Elasticsearch依赖包冲突的解决方案","slug":"Elasticsearch依赖包冲突的解决方案","date":"2018-01-30T03:04:09.000Z","updated":"2018-02-28T01:26:48.883Z","comments":true,"path":"2018/01/30/Elasticsearch依赖包冲突的解决方案/","link":"","permalink":"http://kplxq.github.io/2018/01/30/Elasticsearch依赖包冲突的解决方案/","excerpt":"Guava是谷歌开源的一个工具类库，有很多实用的工具类广为流传，在Java1.7往后的版本也有很多基于Guava类库的工具类进行的优化，很多中间件也依赖Guava用来优化代码，但是Guava的版本升级很快，大版本号升级时，有些类库是不向下兼容的，本文要说的就是在Elasticsearch和HBase中Guava版本不一致导致的依赖冲突的解决方案","text":"Guava是谷歌开源的一个工具类库，有很多实用的工具类广为流传，在Java1.7往后的版本也有很多基于Guava类库的工具类进行的优化，很多中间件也依赖Guava用来优化代码，但是Guava的版本升级很快，大版本号升级时，有些类库是不向下兼容的，本文要说的就是在Elasticsearch和HBase中Guava版本不一致导致的依赖冲突的解决方案 Talos的应用架构中，我们是在talos-storage组件中将链路日志的数据存储到HBase和Elasticsearch，在开发过程中，遇到ES和HBase因依赖的Guava包冲突导致工程启动报错。 HBase版本号：1.2.0 (依赖Guava12.0.1)Elasticsearch版本号：2.4.0 (依赖Guava10.0.1)Java：1.7 Elasticsearch官方对于这个问题还是很重视的，关于是否要移除Guava依赖，在github issue上还有一些激烈的讨论，附上issue链接，因为开发者的重视，在Elasticsearch5.0.0版本的release note中，公开已经将对guava的依赖去掉了。不过在我们开发talos-storage的时候，Elasticsearch还没有发布5.0.0的Release，因此当时是参考官网的一篇指导文档To shade or not to shade，官网是以joda为例，guava的shade方式也如出一辙。 后面配置文件很长，所以先给出结论：1、如果你现在还处于调研阶段，遇到了ES和HBase或其他组件对Guava的依赖冲突，建议转去调研Elasticsearch的最新RELEASE版本。2、如果你和我一样，产线已经部署并运行着Elasticsearch5.0.0以下的版本，那你有两种选择，一种是将Elasticsearch升级到5.0.0，另一种是使用shade的方式，以下将分别介绍这两种方案的实施步骤。 升级Elasticsearch (2.x至5.x)Elasticsearch的大版本升级是需要停止所有节点后再重启的，不支持波浪升级，在升级之前需要做的几件事：1、使用Elasticsearch Migration Plugin插件来排除潜在风险。2、产线环境搞之前一定要在测试环境先验证成功。3、备份数据，备份数据，备份数据。 以下是升级的步骤：1、为避免一个节点关闭后分片之间的数据复制导致的I/O浪费，在关闭节点前先Disable shard allocation 123456PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;cluster.routing.allocation.enable&quot;: &quot;none&quot; &#125;&#125; 2、将flush改为同步操作，分片的恢复会快很多。 1POST _flush/synced 3、逐个节点关闭并升级，这个看你最初安装es是什么方式的，如果是参考taolo的Elasticsearch部署文档，那依然参考这篇博客，只需在下载时使用最新RELEASE的即可。4、升级所有的插件，用elasticsearch-plugin脚本即可。5、启动各个节点，待所有节点均已启动成功，并且status返回的是yellow，表明所有分片已经恢复。6、重新启用分片分配123456PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;cluster.routing.allocation.enable&quot;: &quot;all&quot; &#125;&#125; 7、这时候Elasticsearch已经升级完成了，并可以正常工作了，不过为了集群更快的恢复，建议还是等到status返回green，再进行后续工作。 不升级Elasticsearch，使用shade的方式避免冲突坐享其成talos-storage组件已经实践过并已经发布RELEASE的一个talos-es-shaded，将对elasticsearch的依赖改为如下即可： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;joda-time&lt;/groupId&gt; &lt;artifactId&gt;joda-time&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;12.0.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.kxjf.talos&lt;/groupId&gt; &lt;artifactId&gt;talos-es-shaded&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt; 自力更生创建一个新的maven工程，pom文件如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;groupId&gt;my.elasticsearch.test&lt;/groupId&gt;&lt;artifactId&gt;es-shaded&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;properties&gt; &lt;elasticsearch.version&gt;2.0.0-beta2&lt;/elasticsearch.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;$&#123;elasticsearch.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt; &lt;artifactId&gt;shield&lt;/artifactId&gt; &lt;version&gt;$&#123;elasticsearch.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;elasticsearch-releases&lt;/id&gt; http://maven.elasticsearch.org/releases &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;daily&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;relocations&gt; &lt;relocation&gt; &lt;pattern&gt;org.joda&lt;/pattern&gt; &lt;shadedPattern&gt;my.elasticsearch.joda&lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;relocation&gt; &lt;pattern&gt;com.google.guava&lt;/pattern&gt; &lt;shadedPattern&gt;my.elasticsearch.guava&lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;/relocations&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot; /&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 在执行mvn clean install 后，依赖将变成 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;my.elasticsearch.test&lt;/groupId&gt; &lt;artifactId&gt;es-shaded&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;joda-time&lt;/groupId&gt; &lt;artifactId&gt;joda-time&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;12.0.1&lt;/version&gt;&lt;/dependency&gt; 比如你还要使用2.9.2版本的joda，照常引入org.joda.time.DateTime即可，若你要使用shaded版本中的Joda，引入my.elasticsearch.joda.time.DateTime即可，不过这种做法并不被建议哈，代码如下： 12345CodeSource codeSource = new org.joda.time.DateTime().getClass().getProtectionDomain().getCodeSource();System.out.println(&quot;unshaded = &quot; + codeSource);codeSource = new my.elasticsearch.joda.time.DateTime().getClass().getProtectionDomain().getCodeSource();System.out.println(&quot;shaded = &quot; + codeSource); 将会输出： 12unshaded = (file:/path/to/joda-time-2.1.jar &lt;no signer certificates&gt;)shaded = (file:/path/to/es-shaded-1.0-SNAPSHOT.jar &lt;no signer certificates&gt;) End.","categories":[],"tags":[{"name":"elasticsearch guava","slug":"elasticsearch-guava","permalink":"http://kplxq.github.io/tags/elasticsearch-guava/"}],"keywords":[]},{"title":"震惊！机器人竟然真的开战了，有图有真相！","slug":"震惊！机器人竟然真的开战了，有图有真相！","date":"2018-01-24T07:38:08.000Z","updated":"2018-02-08T07:49:52.548Z","comments":true,"path":"2018/01/24/震惊！机器人竟然真的开战了，有图有真相！/","link":"","permalink":"http://kplxq.github.io/2018/01/24/震惊！机器人竟然真的开战了，有图有真相！/","excerpt":"是我把他带到这个世界， 一个能够思考与感知的机器人。 现在我宣布： 你被终结了！","text":"是我把他带到这个世界， 一个能够思考与感知的机器人。 现在我宣布： 你被终结了！ 2018年1月24日下午，开普勒鑫球杯机器人编程大赛正式开赛，共有20个团队参加，让我们再来一起回顾下吧。（注：源码已开放下载，请拖到文末） 赛前，小伙伴们交流各自机器人的编程算法，设计思想。抢血包、躲地雷，锁定追踪攻击…… 比赛现场，观战。有一个猥琐的机器人“梵高”……，应了那句话：要么不干，要么干出风格！大写的服啊！后续将邀请他来讲讲他诡异的算法。 经过小组赛、晋级赛、决赛等一系列激烈的PK后，各大奖项终于出炉了，一起来看看吧。 ———— 我是分割线 ———— 特别奖获得者：由向日葵、暴风、秋名山、堡垒四个团队获得。 猜猜以上哪个团队创造了机器人“梵高” 第三名！团队名：rookie，参赛机器人：瓦力 第二名！团队名：极限特工，参赛机器人：代号47 第一名！团队名：A&amp;V，参赛机器人：A&amp;V 胜利者的拥抱！爱的拥抱！全场欢呼“在一起”！ 特别感谢：南溪同学 。感谢南溪同学编写了机器人编程大赛的沙盒，为大家提供了一个技术切磋、华山论码的机会，无码诚可贵，有码价更高啊！ 附：所有参赛团队及参赛机器人 比赛沙盒源代码已开源，微信公众号回复“机器人”，即可获取源代码地址。 后续文章会讲解关于机器人PK过程中用到的一些经典的编程思想、编程算法，欢迎关注，交流。也欢迎各位提交自己的机器人，一起来PK！点击公众号菜单“关于我们”即可联系我们哦！","categories":[],"tags":[],"keywords":[]},{"title":"怎样监控Kubernetes容器","slug":"怎样监控Kubernetes容器","date":"2018-01-17T03:53:50.000Z","updated":"2018-01-19T04:47:23.083Z","comments":true,"path":"2018/01/17/怎样监控Kubernetes容器/","link":"","permalink":"http://kplxq.github.io/2018/01/17/怎样监控Kubernetes容器/","excerpt":"怎样监控Kubernetes容器一、容器的运行方式与VM和HOST的差异Kubernetes是现在最流行的容器编排系统，容器与VM和HOST有着显著不同。怎样对k8s平台上的容器进行监控？首先需要注意容器的运行方式与VM和HOST的不同：","text":"怎样监控Kubernetes容器一、容器的运行方式与VM和HOST的差异Kubernetes是现在最流行的容器编排系统，容器与VM和HOST有着显著不同。怎样对k8s平台上的容器进行监控？首先需要注意容器的运行方式与VM和HOST的不同： 运行实例从宏观层面迁移到微观层面容器运行在私有网络中，通常情况下与外部网络隔离。怎样从外部网络进入到容器私有网络、获取容器的监控数据？ 运行实例从静态、长生命周期转变成动态、短生命周期HOST和VM一般是静态IP地址，一旦开机、长期运行。而容器的IP是动态分配的，其创建、销毁、扩容、缩容非常频繁。如何及时发现新创建的容器、获取到它们的监控数据、并在仪表盘上恰当的展现出来？ 二、容器监控方案概述为了解决上述问题，kubernetes、promethues、influxdata等开源组织相继发布了一些容器监控工具和方案。例如：kubernetes 的 heapster+influxdb+grafana，prometheus的prometheus+alertmanager，influxdata的telegraf+influxdb+kapacitor。 我采用的是第一种，即： heapster+influxdb+grafana，实现简单、效果较好。 其中，heapster是k8s容器状态的收集、导出工具，influxdb是一种时序数据库，grafana是一种数据展示和报警系统。 heapster能导出当前时间点的所有容器的状态信息，解决了容器监控信息的采集和导出问题；grafana是功能强大的数据展示和报警工具，它的展示系统支持变量、模板、正则匹配、标签等功能，能把瞬息万变的容器信息有效组织、展示出来，报警系统支持多种方式、还可以基于webhook自己开发，实现短信报警等功能。 三、容器监控部署概述（由于字数限制，不详述，具体请参考官方文档；所有组件都基于容器部署） 部署influxdbhttps://github.com/influxdata/influxdb 部署heapsterhttps://github.com/kubernetes/heapster/tree/master/deploycommand加sink参数，包含influxdb的地址、用户名、密码，例如：–sink=influxdb:http://influxdb.default:8086?db=heapster&amp;user=heapster&amp;pw=1234 部署grafanahttps://github.com/grafana/grafana部署后请设置数据源datasource，加入influxdb。 四、设置仪表盘Grafana的仪表盘也就是监控数据的展示界面，可以自己设计，还可以导出共享给别人。我设计了一个容器监控的仪表盘，共享在grafana网站上，地址是：https://grafana.com/dashboards/3649 在namespace下拉框可以选择k8s容器的命名空间，在pod_name下拉框可以选择容器的匹配名称（前面几个字符或者全名都可以）。 五、建立容器报警我共享的仪表盘含有两个变量：namespace和pod_name，这样的仪表盘叫：模板。Grafana目前不支持在模板里创建报警。为了创建报警，我们需要再建一个不带变量的仪表盘，然后在时序图的Alert菜单里设置报警，设置报警阀值、通道、内容等信息： 六、建立报警一览图Grafana自带一个报警管理页面： 我们可以设计一个更清楚的报警一览图，可参考我共享的模板，地址是：https://grafana.com/dashboards/3489","categories":[],"tags":[],"keywords":[]},{"title":"机器人编程大赛的沙盒源代码正式开放提供下载啦！","slug":"机器人编程大赛的沙盒源代码正式开放提供下载啦！","date":"2018-01-15T08:25:12.000Z","updated":"2018-01-16T09:17:45.969Z","comments":true,"path":"2018/01/15/机器人编程大赛的沙盒源代码正式开放提供下载啦！/","link":"","permalink":"http://kplxq.github.io/2018/01/15/机器人编程大赛的沙盒源代码正式开放提供下载啦！/","excerpt":"怎样才能搞一个又有趣又能提升技术能力的活动？程序员年底团建怎么搞？想搞编程大赛，没有好的方案，没有沙盒？","text":"怎样才能搞一个又有趣又能提升技术能力的活动？程序员年底团建怎么搞？想搞编程大赛，没有好的方案，没有沙盒？ 开普勒鑫球杯机器人编程大赛的沙盒源代码（V1.0）正式对外开放，提供下载啦。 用你的思想控制硝烟弥漫的战场，利用牛逼的算法，牛逼的策略，创造属于你自己的无敌机器人！抢血包！躲地雷！群战！看看究竟谁才是You Xi Zhi King吧！ 在2.0版本的沙盒中，还支持多人PK、组队PK模式，支持传送门、天神下凡、凌波微步等多种更为先进的技能噢，后续也会开放源码，尽情继续关注。 QQ交流群：637375352微信公众号回复“机器人”，即可获取源代码地址。 参考阅读：玩游戏的逆天新姿势，屌炸天啊！！","categories":[],"tags":[{"name":"机器人编程","slug":"机器人编程","permalink":"http://kplxq.github.io/tags/机器人编程/"}],"keywords":[]},{"title":"玩游戏的逆天新姿势，屌炸天啊！！","slug":"玩游戏的逆天新姿势，屌炸天啊！！","date":"2018-01-12T08:24:44.000Z","updated":"2018-01-16T09:18:19.397Z","comments":true,"path":"2018/01/12/玩游戏的逆天新姿势，屌炸天啊！！/","link":"","permalink":"http://kplxq.github.io/2018/01/12/玩游戏的逆天新姿势，屌炸天啊！！/","excerpt":"打游戏，可以有很多姿势，比如下面这样的：","text":"打游戏，可以有很多姿势，比如下面这样的： 但对于程序员来说，这些还是太低级太Low啊，有本事，有本事你不要用手啊！！！你不要用手啊！！！不用手啊！！！手啊！！！啊！！！叮~叮~叮~ 就是下面酱紫滴！开普勒鑫球杯机器人大战编程竞赛，用你的思想控制硝烟弥漫的战场，利用牛逼的算法，牛逼的策略，创造属于你自己的无敌机器人！抢血包！躲地雷！群战！1月17日，现场PK，看看究竟谁才是You Xi Zhi King，参加还有大奖拿噢，是不是很兴奋！！！！ 最最重要的是，下周一（01月15号），我们还会开放机器人挑战赛沙盒的 源 代 码，一起来群P啊！敬请关注！（现场比赛暂时只限内部，欢迎关注开普勒鑫球，后续还有更多有趣的活动，还有福利拿噢）","categories":[],"tags":[{"name":"机器人编程","slug":"机器人编程","permalink":"http://kplxq.github.io/tags/机器人编程/"}],"keywords":[]},{"title":"从一段臭名昭彰却又广为人知的代码说起……","slug":"从一段臭名昭彰却又广为人知的代码说起……","date":"2018-01-10T08:24:26.000Z","updated":"2018-01-16T09:18:42.081Z","comments":true,"path":"2018/01/10/从一段臭名昭彰却又广为人知的代码说起……/","link":"","permalink":"http://kplxq.github.io/2018/01/10/从一段臭名昭彰却又广为人知的代码说起……/","excerpt":"作为一名程序猿，谁没挖过几个坑，坑坑都是泪啊…… ，农历年的年底了，鸡年要走了，狗年要来了，各位码工动代码时千万悠着点，三思而后行，可不能鸡飞狗跳啊！2018年，愿世界和平，天下无坑！今天就来和大家聊聊一个简单的并发编程的坑，","text":"作为一名程序猿，谁没挖过几个坑，坑坑都是泪啊…… ，农历年的年底了，鸡年要走了，狗年要来了，各位码工动代码时千万悠着点，三思而后行，可不能鸡飞狗跳啊！2018年，愿世界和平，天下无坑！今天就来和大家聊聊一个简单的并发编程的坑， 各位编码过程中，遇到过什么坑，欢迎留言，交流分享！","categories":[],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"http://kplxq.github.io/tags/并发编程/"}],"keywords":[]},{"title":"开普勒鑫球全链路监控系统Talos正式开源","slug":"开普勒鑫球全链路监控系统Talos正式开源","date":"2017-12-15T15:17:13.000Z","updated":"2018-01-06T11:36:45.688Z","comments":true,"path":"2017/12/15/开普勒鑫球全链路监控系统Talos正式开源/","link":"","permalink":"http://kplxq.github.io/2017/12/15/开普勒鑫球全链路监控系统Talos正式开源/","excerpt":"2017年12月24日，开普勒鑫球的开源社区正式对外发布了其首个开源项目- Talos。","text":"2017年12月24日，开普勒鑫球的开源社区正式对外发布了其首个开源项目- Talos。 Talos是一个大数据全链路监控系统，由开鑫金服科技团队自主研发，供业界下载使用。开鑫金服科技团队期望通过这种开源的方式促进行业交流、发展，共同进步。 Talos以Google Dapper论文为理论基础，参照Twitter Brave的实现方案，面向当今互联网复杂的环境，基于客户端探针上报应用调用链相关日志，通过ElasticSearch引擎和HBase大数据存储技术，对海量请求进行实时链式跟踪、预警，能快速定位问题根源，有效保障了系统的稳定运行，提升了客户体验。 开鑫金服科技团队通过5年的磨砺，自主研发了多项先进的框架和平台。2017年12月24日，开鑫金服公司成立5周年，为迎接这有意义的一天，团队决定将Talos项目贡献给开普勒鑫球的开源社区；同时也期望更多的技术爱好者加入到开源社区中来，共同分享、交流。开普勒鑫球开源社区后续还会开源更多优质领先的项目，敬请期待！ 开源地址（或点击开普勒鑫球公众号菜单【开源项目】直接访问）：","categories":[],"tags":[{"name":"talos","slug":"talos","permalink":"http://kplxq.github.io/tags/talos/"}],"keywords":[]},{"title":"如何设计一个大数据全链路监控系统","slug":"【纯干货分享】如何设计一个大数据全链路监控系统","date":"2017-12-15T15:07:13.000Z","updated":"2018-01-06T11:37:31.032Z","comments":true,"path":"2017/12/15/【纯干货分享】如何设计一个大数据全链路监控系统/","link":"","permalink":"http://kplxq.github.io/2017/12/15/【纯干货分享】如何设计一个大数据全链路监控系统/","excerpt":"纯干货分享！ 大数据全链路监控系统架构设计内部PPT来啦，赶紧保存。 后续开普勒鑫球还会陆续发布一系列文章逐一展开详细讲解，手把手教你，干货满满，敬请关注吧！","text":"纯干货分享！ 大数据全链路监控系统架构设计内部PPT来啦，赶紧保存。 后续开普勒鑫球还会陆续发布一系列文章逐一展开详细讲解，手把手教你，干货满满，敬请关注吧！","categories":[],"tags":[{"name":"talos","slug":"talos","permalink":"http://kplxq.github.io/tags/talos/"}],"keywords":[]},{"title":"部署文档","slug":"部署文档","date":"2017-12-15T14:59:56.000Z","updated":"2017-12-22T10:00:06.488Z","comments":true,"path":"2017/12/15/部署文档/","link":"","permalink":"http://kplxq.github.io/2017/12/15/部署文档/","excerpt":"整体部署架构","text":"整体部署架构 部署节点数及基准配置 组件名称 组件类型 建议节点数 基准配置 搜索引擎(elastic search) 中间件 Elastic Search(3) cpu:2C memory:8G 分布式列存储数据库(Hbase) 中间件 CDH Manager(1) cpu:2C memory:4G disk:50G 分布式列存储数据库(Hbase) 中间件 Hbase MasterServer(1) cpu:2C memory:4G disk:50G 分布式列存储数据库(Hbase) 中间件 Hbase RegionServer(2) cpu:2C memory:4G disk:150G(留存30天数据) 分布式队列服务器(Kafka) 中间件 Kafka Server(3) cpu:2C memory:4G disk:20G 分布式配置管理服务器(zookeeper) 中间件 Zookeeper Server(3) cpu:2C memory:4G talos-storage 自有组件 2 cpu:2C memory:4G disk:20G talos-dashboard 自有组件 1 cpu:2C memory:4G disk:20G 部署步骤hosts配置在部署前请维护整个集群环境的hosts，并推送到每个部署节点 12345678910111213141516171819202122#es 集群192.168.99.101 es1192.168.99.102 es2192.168.99.103 es3# kafka集群192.168.99.104 kafka1 zk1192.168.99.105 kafka2 zk2192.168.99.106 kafka3 zk3# hbase集群192.168.99.107 hbase1192.168.99.108 hbase2192.168.99.109 hbase3192.168.99.110 cdhmaster# talos-dashboard192.168.99.111 talos-dashboard# talos-storage192.168.99.112 talos-storage1192.168.99.113 talos-storage2 中间件安装中间件版本信息如下: 中间件名称 版本 备注 搜索引擎 Elastic Search-2.4.0 3个节点 分布式列存储数据库 Hbase-1.2.0(CDH-5.8.0) 4个节点 CDH Manager(1) Hbase MasterServer() Hbase RegionServer(2) 分布式队列服务器 Kafka-2.11-0.10.0.1 3个节点 分布式文件系统 Hdfs-2.6.0(CDH-5.8.0) 内嵌至Hbase部署中 分布式配置管理服务器 Zookeeper-3.4.5(CDH-5.8.0) - Kafka1、中间件安装另附文档Kafka部署文档2、创建Talos系统在kafka集群所用的topic 配置如下： topic名称：talos-open-sourcepartition数目：1024replication数据：1 (无需备份) 脚本如下： 1./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1024 --topic talos-open-source Elasticsearch1、中间件安装另附文档Elasticsearch部署文档2、创建Talos系统elasticsearch索引 配置如下：index： talosmapping: trace刷新时间：5s分片数：5备份：0 123456789101112131415curl -XPOST http://es1:9200/talos -d &apos;&#123; &quot;settings&quot; : &#123; &quot;number_of_replicas&quot;: &quot;0&quot;, &quot;number_of_shards&quot;: &quot;5&quot;, &quot;refresh_interval&quot;: &quot;5s&quot; &#125;, &quot;mappings&quot; : &#123; &quot;trace&quot; : &#123; &quot;properties&quot; : &#123; &quot;traceid&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;index&quot; : &quot;not_analyzed&quot; &#125;, &quot;contents&quot; : &#123; &quot;type&quot; : &quot;string&quot;&#125; &#125; &#125; &#125;&#125;&apos; HBase1、中间件安装参考HBase部署文档2、创建Talos系统Hbase表 配置如下：表名：tracerowkey: id （talos系统中的traceId)列族：span （talos系统中的不同的spanId即为该列族下的不同列）数据失效时间：30天 脚本如下（在hbase shell中执行）： 1create &apos;trace&apos;, &#123;NAME=&gt;&apos;id&apos;, TTL=&gt;&apos;2592000&apos;&#125;,&#123;NAME=&gt;&apos;span&apos;, TTL=&gt;&apos;2592000&apos;&#125; 系统自有组件安装talos-dashboard部署前请确认hosts已更新 1、下载talos-dashboard.war2、将talos-dashboard包放到tomcat/webapps路径下3、启动tomcat4、浏览器打开talos-dashboard，地址：http://talos-dashboard:8080/talos-dashboard/index.html 123cd /usr/share/tomcatwget https://gitee.com/lhldyf/talos-readme/raw/master/talos-dashboard.warservice tomcat start talos-storage部署前请确认hosts已更新 1、下载talos-storage.zip2、解压至/usr/share/talos-storage3、启动talos-storage 12345cd /tmpwget https://gitee.com/lhldyf/talos-readme/raw/master/talos-storage.zipunzip /tmp/talos-storage.zip -d /usr/sharecd /usr/share/talos-storagesh bin/start.sh;tailf logs/stdout.log","categories":[],"tags":[{"name":"talos","slug":"talos","permalink":"http://kplxq.github.io/tags/talos/"}],"keywords":[]},{"title":"Kafka部署文档","slug":"Kafka部署文档","date":"2017-12-15T13:01:57.000Z","updated":"2017-12-22T09:57:24.639Z","comments":true,"path":"2017/12/15/Kafka部署文档/","link":"","permalink":"http://kplxq.github.io/2017/12/15/Kafka部署文档/","excerpt":"机器列表 节点 建议host Node1 kafka1 zk1 Node2 kafka2 zk2 Node3 kafka3 zk3","text":"机器列表 节点 建议host Node1 kafka1 zk1 Node2 kafka2 zk2 Node3 kafka3 zk3 逻辑拓扑 部署步骤组件下载Download KafkaDownload Zookeeper 下载链接若失效，请参考官网最新链接 KafkaZookeeper Zookeeper安装 将zookeeper解压缩到/opt/zookeeper目录下 cp /opt/zookeeper/conf/zoo_sample.cfg /opt/zookeeper/conf/zoo.cfg 修改配置文件: vim /opt/zookeeper/conf/zoo.cfg 文件末尾加上 1234autopurge.purgeInterval=1server.1 = kafka1:2888:3888server.2 = kafka2:2888:3888server.3 = kafka3:2888:3888 /opt/zookeeper/bin/zkServer.sh start 启动 zk Kafka安装1、 将kafka解压缩到/opt/kafka目录下2、 修改/opt/kafka/config/consumer.properties文件，将zookeeper.connect=127.0.0.1:2181改成zookeeper.connect=zk1:2181,zk2:2181,zk3:21813、修改/opt/kafka/config/producer.properties文件，将bootstrap.servers=localhost:9092改成bootstrap.servers=kafka1:9092,kafka2:9092,kafka3:90924、修改/opt/kafka/config/server.properties文件，修改以下key： 123broker.id=2 （2表示node2，node1就是1，node3就是3）listeners=PLAINTEXT://127.0.0.1:9092 （本机ip）zookeeper.connect=zk1:2181,zk2:2181,zk3:2181 5、在/tmp/zookeeper目录下，新建一个myid文件，内容是2（node2就写2， node3就写3，同broker.id）6、进入/opt/kafka/bin/目录执行 ./kafka-server-start.sh ../config/server.properties &amp; 启动kafka","categories":[],"tags":[{"name":"talos","slug":"talos","permalink":"http://kplxq.github.io/tags/talos/"}],"keywords":[]},{"title":"Elasticsearch部署文档","slug":"Elasticsearch部署文档","date":"2017-12-15T11:57:07.000Z","updated":"2017-12-22T09:57:29.471Z","comments":true,"path":"2017/12/15/Elasticsearch部署文档/","link":"","permalink":"http://kplxq.github.io/2017/12/15/Elasticsearch部署文档/","excerpt":"机器列表 节点 建议host Node1 es1 Node2 es2 Node3 es3","text":"机器列表 节点 建议host Node1 es1 Node2 es2 Node3 es3 环境准备1、JDK1.7+ 节点部署官网有quick-start 注意：Elasticsearch 限制使用非root用户来进行以下操作，如创建用户es 1、下载Elasticsearch2.4.0的zip包2、解压elasticsearch至/usr/share路径3、修改配置文件config/elasticsearch.yml 12345678910111213141516171819202122# es 集群名称，同一个集群名称需一致cluster.name: es-talos# es节点名称，每个节点设置不同，三个节点1/2/3即可node.name: node-1# 节点rack，每个节点设置不同，三个节点1/2/3即可node.rack: r1path.logs: /usr/share/elasticsearch-2.4.0/logs# 节点host，本机host即可network.host: es1http.port: 9200 http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; # 集群部署的另外两个节点discovery.zen.ping.unicast.hosts: [&quot;es2&quot;,&quot;es3&quot;]discovery.zen.minimum_master_nodes: 2# 默认index刷新间隔 index.refresh_interval: 120s# 默认数据副本数index.number_of_replicas: 0# 设置脚本可执行script.inline: true script.indexed: true 4、修改elasticsearch jvm内存大小,建议值为本机物理内存/2修改文件install_path/bin/elasticsearch.in.sh文件中第16行及19行,设置ES_MIN_MEM=物理内存/2(16行)、ES_MAX_MEM=物理内存/2(19)行 5、解压后，使用es用户进行到bin目录下进行启动 1./elasticsearch -d 6、浏览器访问 es1:9200，看到如下响应则启动成功 123456789101112&#123; &quot;name&quot; : &quot;node-1&quot;, &quot;cluster_name&quot; : &quot;es-talos&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;2.4.0&quot;, &quot;build_hash&quot; : &quot;ce9f0c7394dee074091dd1bc4e9469251181fc55&quot;, &quot;build_timestamp&quot; : &quot;2016-08-29T09:14:17Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;5.5.2&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 以上1-6步骤在三个节点上操作完成，并都能看到启动成功的响应，则部署成功。 ES监控插件部署1、 如果在有网络的情况下可以直接执行/usr/share/elasticsearch2.4.0/bin/plugin install mobz/elasticsearch-head2、 如果服务器没有连网下载elasticsearch-head3、 可以通过web服务器打开，如将elasticsearch-head放到tomcat的里面，通过tomcat访问http://es1:8080/elasticsearch-head-master/index.html 部署成功如图：","categories":[],"tags":[{"name":"talos","slug":"talos","permalink":"http://kplxq.github.io/tags/talos/"}],"keywords":[]},{"title":"Talos接入使用说明","slug":"Talos接入使用说明","date":"2017-12-15T11:14:07.000Z","updated":"2017-12-22T10:03:55.924Z","comments":true,"path":"2017/12/15/Talos接入使用说明/","link":"","permalink":"http://kplxq.github.io/2017/12/15/Talos接入使用说明/","excerpt":"运行环境开发环境开发环境需要： JDK1.7+ Maven3.1+","text":"运行环境开发环境开发环境需要： JDK1.7+ Maven3.1+ 依赖引入12345&lt;dependency&gt; &lt;groupId&gt;com.kxjf.talos&lt;/groupId&gt; &lt;artifactId&gt;talos-interceptor&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt; 使用说明这里对一些配置进行解释说明，范例代码移步talos-sample 配置说明必选配置Talos实例化Spring配置Talos实例，配置说明：1、 serviceName：用于区分集群应用，需唯一，建议取机器host2、 collector ：数据收集的实现方法，默认使用日志收集方式，需额外配置logback，系统目前提供这一种收集方式，可自行实现 参考配置： 12345&lt;bean id=&quot;talos&quot; class=&quot;com.kxd.talos.trace.core.Talos&quot;&gt; &lt;constructor-arg type=&quot;String&quot; value=&quot;talos-sample&quot; /&gt; &lt;constructor-arg type=&quot;float&quot; value=&quot;1.0f&quot; /&gt; &lt;constructor-arg ref=&quot;loggingSpanCollector&quot; /&gt;&lt;/bean&gt; logback配置配置logback.xml，需增加trace collector的日志输出配置，有几个自定义的环境配置，说明如下：1、 KAFKA_TOPIC_NAME: kafka推送的topic名称，用于接收日志数据2、 KAFKA_BOOTSTRAP_SERVERS ： kafka集群节点配置，” ip1:port1,ip2:port2…” 的方式配置即可。参考如下： 12345678910111213141516171819202122232425262728293031323334353637383940&lt;appender name=&quot;talosTraceCollectorFileAppender&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;!-- 正在记录的日志文件的路径及文件名 --&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!--日志文件输出的文件名 --&gt; &lt;fileNamePattern&gt;$&#123;LOG_HOME&#125;/$&#123;TALOS_TRACE_LOG_FILE&#125; &lt;/fileNamePattern&gt; &lt;!--日志文件保留天数 --&gt; &lt;MaxHistory&gt;$&#123;LOG_SAVE_DAYS&#125;&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;pattern&gt;%msg%n&lt;/pattern&gt; &lt;/encoder&gt;&lt;/appender&gt;&lt;appender name=&quot;talosKafkaAppender&quot; class=&quot;com.github.danielwegener.logback.kafka.KafkaAppender&quot;&gt; &lt;encoderclass=&quot;com.github.danielwegener.logback.kafka.encoding.LayoutKafkaMessageEncoder&quot;&gt; &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;pattern&gt; %msg&lt;/pattern&gt; &lt;/layout&gt; &lt;/encoder&gt; &lt;topic&gt;$&#123;KAFKA_TOPIC_NAME&#125;&lt;/topic&gt; &lt;keyingStrategyclass=&quot;com.github.danielwegener.logback.kafka.keying.RoundRobinKeyingStrategy&quot; /&gt; &lt;deliveryStrategyclass=&quot;com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy&quot; /&gt;&lt;producerConfig&gt;bootstrap.servers=$&#123;KAFKA_BOOTSTRAP_SERVERS&#125; &lt;/producerConfig&gt; &lt;!-- this is the fallback appender if kafka is not available. --&gt; &lt;appender-ref ref=&quot;talosTraceCollectorAppender&quot; /&gt;&lt;/appender&gt;&lt;appender name=&quot;asyncTalosKafkaAppender&quot; class=&quot;ch.qos.logback.classic.AsyncAppender&quot;&gt; &lt;appender-ref ref=&quot;talosKafkaAppender&quot; /&gt;&lt;/appender&gt;&lt;logger name=&quot;com.kxd.talos.trace.core.collector&quot; level=&quot;ALL&quot; additivity=&quot;true&quot;&gt; &lt;appender-ref ref=&quot;asyncTalosKafkaAppender&quot; /&gt; &lt;appender-ref ref=&quot;talosTraceCollectorFileAppender&quot; /&gt;&lt;/logger&gt; 配置logback.properties，参考如下： 123TALOS_TRACE_LOG_FILE=talos-trace_%d&#123;yyyy-MM-dd&#125;.logKAFKA_BOOTSTRAP_SERVERS=kafka1:9092,kafka2:9092,kafka3:9092KAFKA_TOPIC_NAME=talos-open-source Web应用配置TalosServletFilterSpring配置TalosServletFilter实例，初始化配置filter中的Talos实例，参考如下： 123456&lt;bean id=&quot;httpServerServletFilter&quot; class=&quot;com.kxd.talos.trace.interceptor.server.http.TalosServletFilter&quot;&gt; &lt;property name=&quot;talos&quot; ref=&quot;talos&quot; /&gt; &lt;!-- 配置需要过滤的url,可使用*进行匹配,如有多个,用英文逗号(,)分割 --&gt; &lt;property name=&quot;patterns&quot; value=&quot;/**&quot; /&gt;&lt;/bean&gt; web.xml中增加配置，使用代理模式，参考如下： 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;TraceFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;targetFilterLifecycle&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;targetBeanName&lt;/param-name&gt; &lt;param-value&gt;httpServerServletFilter&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;TraceFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 多线程配置使用约束：1、 多线程使用线程池方式(ThreadPoolExecutor)，不得使用new Thread()模式生成新线程2、 多线程方法必须实现Runable或Callable接口。Talos对spring 线程池ThreadPoolTaskExecutor 做了一层额外封装，收集多线程的相关数据，在使用多线程定义线程池时，额外实例化该类实例，执行线程池方法时，使用该实例，参考如下： 12345&lt;bean id=&quot;talosExecutorService&quot; class=&quot;com.kxd.talos.trace.core.concurrent.TalosSpringThreadPool&quot;&gt; &lt;constructor-arg ref=&quot;threadPoolExecutor&quot; /&gt; &lt;constructor-arg ref=&quot;talos&quot; /&gt;&lt;/bean&gt; 自定义采集配置自定义采集如果需要使用callback模式，需要配置callback的模板如下： TalosCallbackTemplateAOP的拦截有一定的限制性，对于一些无法进行AOP切面拦截的方法入口，如果有采集数据的必要性，Talos提供了callback模式的采集方式，需要新增的配置如下: 123&lt;bean id=&quot;talosCallbackTemplate&quot; class=&quot;com.kxd.talos.core.trace.TalosCallbackTemplate&quot;&gt; &lt;property name=&quot;talos&quot; ref=&quot;talos&quot;/&gt;&lt;/bean&gt; 数据采集说明自定义采集Talos提供两种自定义的数据采集方式,以下两种方式二选一即可 example: 123private void step3() &#123; aopServiceB.step4(); &#125; 这是现有的一个方法，无法通过AOP拦截，但又有数据采集的必要性，Talos系统提供两种实现方案，以下逐一说明： 方案一注入talos，通过start方法和finish方法完成数据采集，注意需要将方法用try..catch捕获异常，并在finally语句中做finish操作。 12345678910111213141516171819@Autowiredprivate Talos talos;private void step3() &#123; Span startSpan = talos.start(&quot;AopServiceA.step3&quot;); try &#123; aopServiceB.step4(); &#125; catch (AppException ae) &#123; startSpan.setExType(&quot;A&quot;); startSpan.setErrorCode(ae.getErrorCode()); throw ae; &#125; catch (Throwable t) &#123; startSpan.setExType(&quot;T&quot;); startSpan.setErrorCode(ErrorCode.ERROR_SERVICE_INTERCEPTOR_INVOKE); throw t; &#125; finally &#123; talos.finish(startSpan); &#125;&#125; 方案二注入TalosCallbackTemplate实例，将原有方法放在TalosCallback的execute方法即可。 123456789101112@Autowiredprivate TalosCallbackTemplate template;private void step3() &#123; template.execute(null, new TalosCallback()&#123; @Override public Object execute(Object request) &#123; aopServiceB.step4(); return null; &#125; &#125; );&#125; 业务数据采集注:为保证可在海量数据中对指定业务调用链进行搜索,建议每个业务均应当进行业务数据的采集,且该业务数据应当可唯一标识某次业务调用 为了更加易于检索出请求链路，业务人员必须在一个调用链中硬编码的方式写入一些业务数据。API为Talos.collect(String key, String value), 相关原则如下： 1、key 为英文字母组成，驼峰命名，需能根据该值知晓其代表的含义，比如userName, userId等。 2、value 长度不超过100，参数值确保可以方便的搜索出唯一一条调用链，或通过多个参数值确认一条调用链。参考使用如下： 1234public void withParam(ParamDto paramDto) &#123; Talos.collect(&quot;userName&quot;, paramDto.getUserName()); aopServiceB.step1();&#125; 多线程数据采集通过Spring注入TalosSpringThreadPool的实例，使用方法参考ThreadPoolTaskExecutor，参考如下： 1234567891011121314151617@Autowiredprivate TalosSpringThreadPool talosExecutorService;public void call2thread() &#123; talosExecutorService.submit(callableService); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; talosExecutorService.submit(callableService2); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125;","categories":[],"tags":[{"name":"talos","slug":"talos","permalink":"http://kplxq.github.io/tags/talos/"}],"keywords":[]},{"title":"单元测试中mock框架的简单使用","slug":"单元测试中mock框架的简单使用","date":"2016-07-31T08:24:05.000Z","updated":"2018-01-16T09:18:11.693Z","comments":true,"path":"2016/07/31/单元测试中mock框架的简单使用/","link":"","permalink":"http://kplxq.github.io/2016/07/31/单元测试中mock框架的简单使用/","excerpt":"为什么要单元测试： 帮助理解需求：开发人员在编写测试代码的时候，可以更加清楚的了解代码的结构和业务逻辑。 尽早的发现bug：在&lt;快速软件开发&gt;这本书中指出，根据大量的研究数据证明：最后才修改一个bug的代价是在bug产生时修改它的代价大10倍。 提高局部代码的质量：保证局部代码质量，我们才能保证各个依赖你的其他模块的代码质量。 成本：这里说的测试成本是相对而言的，比如:对于集成测试的复杂环境部署，单元测试显得相对简单点。笔者简单了解了下笔者公司的开发写个单元测试平均在0.5H左右。 单元测试可以被复用：一劳永逸。一些固化的功能模块，只要我们写好单元测试后，以后基本不需要调整，为实现单元测试自动化打好了基础。","text":"为什么要单元测试： 帮助理解需求：开发人员在编写测试代码的时候，可以更加清楚的了解代码的结构和业务逻辑。 尽早的发现bug：在&lt;快速软件开发&gt;这本书中指出，根据大量的研究数据证明：最后才修改一个bug的代价是在bug产生时修改它的代价大10倍。 提高局部代码的质量：保证局部代码质量，我们才能保证各个依赖你的其他模块的代码质量。 成本：这里说的测试成本是相对而言的，比如:对于集成测试的复杂环境部署，单元测试显得相对简单点。笔者简单了解了下笔者公司的开发写个单元测试平均在0.5H左右。 单元测试可以被复用：一劳永逸。一些固化的功能模块，只要我们写好单元测试后，以后基本不需要调整，为实现单元测试自动化打好了基础。 当前单元测试遇到的问题：本地测试代价大：笔者所在公司有将近10个系统，有时候开发一个简单的功能，如果不发布到集成环境测试，在本地做单元测试至少需要启动2-3个系统才可以跑起来，经常IDE卡死。而发布到集成环境测试则耗时较长且调式麻烦。 复杂性：开发需要关心各种环境配置项的值，才可以正确的启动系统做单元测试，如：证书、密钥、验签等安全配置。特别是像笔者所在的这种金融领域，各种安全配置、银行调用URL配置。 不可控性：跨公司、跨部门、跨系统的接口调用，导致单元测试的效率和结果不可控。 异常分支测试：笔者做的金融系统，有很多和银行交互的接口，有的时候需要连接一下银行的测试环境测试下“难于上青天啊”(小银行会简单的配合你下，大银行根本不鸟你，给你一个地址你自己玩去吧)，更别说给你返回个异常数据了，连正常的数据都返回不了。另外比如：并发、系统压力测试(有的时候会选择硬编码将调用外部系统的地方写死，压力测试后再改回来上线，但是容易漏改、耗费人力、不能复用)，还有当前很流行的各种分布式、大数据的单元测试都比较困难。 常见的解决方案下面笔者简单分析下上述问题常见的解决方案(可能有更好的方案，这里笔者还没有想到,可以一起交流学习)： 问题1-solve. 部署一套稳定的环境：专门给开发人员单元测试连接调用，但是也存在一些问题，比如我们经常发现使用dubbo这样的RPC调用框架，会出现消费者和服务者混乱，因为共用一套环境大家都把自己的服务注册上去了。数据库数据会被别的开发修改，调式半天发现数据被别的开发修改了，这种情况痛苦的一比。 最理想的是一人一套环境，只有财大气粗的公司如此了。 问题2-solve. 统一开发目录和配置项：对于安全配置的证书这些问题比较好解决，所有开发共用一套单元测试环境变量配置，证书路径和url都统一。不同操作系统不同模板 问题3-solve. 万年难：问题三这个就比较难搞定了，例如你正在测试自己的case，突然你调用的B部门的服务出问题了，你会经常听到类似：我擦，服务被关闭了？他们在发布新版本？返回的数据不对啊、怎么用户不存在？我靠，他们又刷库了？。一等就是千万年，无法忍受。 当然笔者也曾经试着将这些接口调用全部写死了。直接new一个结果返回，然而细 心的人提交代码的时候可能会检查下，不细心的则深挖坑啊。。。。。。 问题4-solve. 写死返回结果：在需要异常场景的时候，注释掉调用代码，写死返回结果。基本和3类似。 后来基于3和4的解决思路，慢慢的就演变出了一种专门解决这些场景的框架：Mock框架。这也符合笔者一直崇尚的理念：**业务驱动开发，有需求就会诞生解决方案**。 Mock框架初接触笔者这里简单的说下自己的mock实现，如果有不足的地方还希望各路大神多多指教，相互学习成长。 选择TestNG: 首先做单元测试当然少不了junit或者testng了(也有NB的公司有自己的测试框架，这些公司呢想必也都是业务驱动逼迫自己去搞的)，笔者这里就不阐述着两个测试框架的区别了，网上各种帖子，总之适合自己、用的熟练、懂得原理就可以。那笔者这里选择的是testng。 选择Jmockit作为mock框架：另外一个就是mock框架的选型。当前的江湖中，mock框架已经有很多门派了，但是万变不离其宗，他们要么是JDK的动态代理、要不就是CGLIB的动态代理生成新的类。比如：easymock 、mockito、jmock等已经风生水起了，但是笔者认为这样的实现原理决定了它的局限性，比如：final方法、构造方法、不能被覆写的方法这些就不能被mock了。 思路到这里暂停一些，我们来回忆下最初的我们： 记得很多年前，我们在刚学java的时候，大神们就教导我们学习java，首先我们得知道一个java文件是怎么最终被机器执行的。 我们写了一个Hello.java 然后通过cmd命令javac Hello.java经过javac的编译器后完成了对代码的词法分析、语法分析、抽象语法树，然后得到一个Hello.class,这部分是在JVM外面的完成的。 然后由JVM类加载到内存中(这里笔者就不叙述加载的过程了，网上可以搜到很多相关文章，再说笔者自己了解的也是皮毛)。到JVM以后，然后JVM翻译成机器码执行。 有了这样的背景知识，再让我们思考如何mock一个类，我们自然的会想到的去修改这个类中在JVM中的字节码，这样就没有什么不可以mock的了。我们知道从JDK1.5开始就提供了java.lang.instrument包，其中提供了修改JVM中已加载类的重定义入口即java.lang.instrument.Instrumentation#redefineClasses(ClassDefinition...)方法。 那当前江湖中有没有这样的一个框架可以和我们的思路很符合呢？笔者google到了这样的一种框架，那就是JMockit(笔者一直认为框架只要适合自己能够满足业务场景就好，无需过多的去追求时髦)。 Jmockit简单介绍： JMockit：是googlecode上的一个项目衍生而来，现在已经有了自己的独立门户网站，官网介绍其是基于asm库来修改java的字节码从而达到篡改类的行为的mock工具。通过JDK提供的类重定义方法：java.lang.instrument.Instrumentation#redefineClasses(ClassDefinition…)作为修改JVM中类的定义的入口。 这样mock框架就定了。是时候我们设计下我们蓝图了。 融合TestNG和JMockit 框架设计： AbstractMockBase：mock类的基类，为以后扩展预留。 TestAMock: 具体的Mock实现类。 mockContext.xml：所有的mock类集中于xml中进行管理，然后写了一个JMockitBeanFactory加载这些类。方便以后统计、修改。 JMockitBase：所有需要用到mock的单元测试类继承此类，提供getMockBean方法。 AbstractDataProvider：提供从xml读取单元测试源数据入口。 TestNGIInvokedMethodListener：testng中的IInvokedMethodListener监听接口实现，完成MockInfo注解的实现。 注：JMockit是通过类TestNGRunnerDecorator实现Testng的两个接口：IInvokedMethodListener，IExecutionListener来实现和TestNG交互的.。 另外我们mock spring容器中bean的时候，一定要拿到被代理前的原始类。方法如下： 编码实战版本 Testng：6.8 Jmockit：1.21 其他的Spring依赖各位随意吧. 场景一: mock原有的接口返回 笔者这里以金融系统中查询工作日这样的接口举例，通常这种查询我们都会调用一个单独的统一辅助系统去查询某天是否是工作日，然后依次来判断下一步的逻辑。但是笔者希望他永远返回是工作日，且不用配置远程调用的任何信息。(最直接的就是在调用出修改代码，写死返回值，这样虽然可以解决问题但是就像前面笔者说的，复用性不强且容易出问题) 下面笔者列举出Mock主要的代码实现,完整代码笔者会稍后上传到github上。 a. 工作日查询接口 b. 工作日查询接口实现编写WorkDayAssistant的mock类:空父类，以后扩展用 Mock类(类的部分mock)(Jmock分类局部mock和全部mock)基金购买接口基金购买接口实现 c. 自定义MockInfo注解-用于定义当前test method 运行时需要哪些bean被mock 运行测试 运行结果： 场景二: 从xml获取数据源 对于一些模块和功能已经固化的代码，我们希望用固定的数据在每个迭代版本中都可以得到固定的结果，笔者这里拿金融系统中常见的绑卡场景为例。 定义xml格式 DataProvider编写 数据获取使用(这里取数据比较恶心，要从map中get，笔者计划有时间改成JavaBean字段映射) 运行结果： 然已经取到数据了，那后面的测试和结果校验随意吧。 总结：当前笔者只是简单介绍了框架的简单使用和集成，后续笔者将抽时间将Jmockit的原理、详细使用方法、当期框架设计优化改进的地方再发表出来和大家一起学习交流。 作者：猎狐，就职于开鑫金服，主要负责Java Web方向的开发工作。","categories":[],"tags":[{"name":"mock框架","slug":"mock框架","permalink":"http://kplxq.github.io/tags/mock框架/"}],"keywords":[]},{"title":"React一小时入门","slug":"React一小时入门","date":"2016-07-27T08:23:36.000Z","updated":"2018-01-16T09:18:28.180Z","comments":true,"path":"2016/07/27/React一小时入门/","link":"","permalink":"http://kplxq.github.io/2016/07/27/React一小时入门/","excerpt":"React是Facebook开发的js库，不仅影响了其他前端库的开发思路，而且还引申出React Native等技术，在开源世界引起了极大的反响。 Facebook为什么要花费大量的精力开发React，为了解决什么问题，以及如何解决问题，我们将就这些问题做一些简单的讨论和学习。","text":"React是Facebook开发的js库，不仅影响了其他前端库的开发思路，而且还引申出React Native等技术，在开源世界引起了极大的反响。 Facebook为什么要花费大量的精力开发React，为了解决什么问题，以及如何解决问题，我们将就这些问题做一些简单的讨论和学习。 React背景Facebook在开发广告系统时发现，因为他们非常庞大的代码库，导致前端的MVC架构非常复杂难以维护。每当开发新需求时，系统复杂度成倍增长，代码非常脆弱且执行结果不可预测。所以Facebook认为MVC架构不适合开发大规模的前端应用，其中很重要的原因是应用中的模型（M）和视图（V）之间的双向数据绑定导致前端代码复杂度迅速提高，难以理解和调试，极大地影响Facebook的开发效率。 Facebook给出的解决方案就是React。React在Facebook内部已经试用了多年，效果很好。React另辟蹊径解决前端代码复杂度高的问题，JSX语法甚至被初学者认为不伦不类。但是我在尝试了React之后，我无法自拔地喜欢上了这种单向数据驱动的开发思路。 那么React是解决什么问题的呢，Facebook官网上介绍说： We built React to solve one problem: building large applications with data that changes over time. 即React是用来构建那些数据会随时改变的大型应用。为了构建大型应用，React有两个主要的特点： 简洁代码里非常简单的描述在每个时间点应用应该呈现的样子。当应用数据改变时，React会自动管理UI界面。 声明式当数据发生改变时，React表现的是刷新DOM树。但事实上React仅仅更新发生了变化的那一部分。我们要做的就是构建组件（Component），封装组件。React自动管理组件的生命周期。 React特性React有三大特性：组件化、虚拟DOM和单向数据流。这三大特性是React运行的基础，并且由虚拟DOM衍生出React Native项目。 组件化React允许将代码封装成组件（Component），然后像插入普通HTML标签一样，在页面中插入这个组件(源码参见：https://github.com/xeostream/react-demo/blob/master/helloworld.html)。 上述代码中，变量HelloMessage就是一个组件类。所有的组件类必须有render方法，用于输出组件内容。 组件的用法与原生的HTML标签完全一致，可以任意加入属性。比如，就是在HelloMessage组件中加入message属性，值为“yo，what’up,man?”。组件内部可以通过this.props对象获取组件的属性，this.props.message就是取message属性值。上面代码的运行结果如下。 组件化的最显著特征就是万物皆由组件构成，那么多个组件之间相互通信就是很大的问题。一般解决方式有三种： 使用props，构建通信链 在组件初始化时，保存组件的句柄。在其他组件中使用句柄达到直接访问组件的目的，完成通信 使用PubSub模式 首先第一种方法容易理解，但是在组件嵌套较深的情况下，为达到通信的目的，组件之间相互调用而且组件需要冗余许多不需要的props，不太适合；第二种方法避免了第一种方法的问题，但是需要维护很多变量，也不是非常好的方案；对于第三种方法，PubSub模式有助于组件解耦和代码组织，而且PubSub有很多开源实现。建议组件间通信使用PubSub模式。 虚拟DOM当组件状态改变的时候，React会自动调用组件的render方法重新渲染整个组件的UI。 但是如果这样大面积的操作DOM，性能会是一个很大的问题，所以React实现了虚拟DOM。 虚拟DOM是一个纯粹的JS数据结构，存到内存中，性能很快。React将组件的DOM结构映射到虚拟DOM上，在虚拟DOM上实现了一个高效的diff算法。所以每次当组件的数据更新时，React会通过diff算法找到需要更新的DOM节点，再把修改更新到浏览器实际的DOM节点上。 单向数据流 单向数据流是React推崇的一种应用架构的方式。在React的组件中，我们监听状态的变化，并在组件的声明周期函数里对组件状态做一个的响应和操作，即页面的变化只与状态数据的变更有关。这里展示一种官方的单向数据流实现： （注：Flux由单向数据流扩展而来，React与Flux相互独立，React仅实现Flux架构中的View部分。当然也可以使用其他js库实现这个View。） 这里我们将React组件理解为一个状态机，状态机内部的状态发生了改变，则对外的输出也会发生改变，两者之间的关系是一一对应的，即如果组件的状态数据是确定的话，则组件的输出也是确定的。这点对于前端的测试和DEBUG是非常大的帮助，对减少前端BUG是很有好处的。 我开始不太理解单向数据流的概念，因为搞不清楚单向数据流和MVC的关系。其实单向数据流并不是和MVC在同一层次对系统的抽象，单向数据流表达的是MVC中View和Model之间数据的传递方式。所以这个问题更精确的表达应该是单向数据流和双向数据流之间的对比。 双向数据流常见于Angular1.x等库中，指Model和View可以相互传递数据，且多个Model和View之间传递没有限制，其中传递是代码不需要显式设置监听事件以同步数据，而是Model和View相互绑定。 所以双向数据流的优势是容易理解，在简单系统中开发非常方便；但是缺点是在复杂工程中，多个Model和View之间绑定，保持这种绑定关系极耗性能，经常会导致View无响应，性能急剧下降，而且因为存在多层绑定关系，导致View的Debug几乎不可能。基于以上原因，很多前端框架如angular2.0已经开始将单向数据流作为默认的绑定方式。 React原理上面一直在说React是要解决什么问题的，现在说下React是怎么解决这些问题的？ 传统的web应用，操作DOM一般是直接更新操作或者是大面积页面刷新，这种操作是比较昂贵的。React为了尽量减少对DOM的操作，提供了一种与众不同的方式来更新DOM。就是DOM层之前增加一个轻量级的虚拟DOM，虚拟DOM是React抽象出来的描述真实DOM结构的对象，由虚拟DOM管理真实DOM的更新。 虚拟DOM为保证高效的更新真实DOM，在更新之前增加diff算法计算出真实DOM的最小变更。 在真实DOM树上的节点被称为元素，在虚拟DOM里被称为组件。组件是非常重要的，虚拟DOM是由组件组成。 component 的使用在 React 里极为重要, 因为 components 的存在让计算 DOM diff 更高效。 state、props和render在组件中是非常重要的属性。state、props属性包含定义组件需要的数据。state表示组件当前的状态，当state发生变化时，组件会调用render方法重新渲染。相对于state，props是组件初始化需要的数据，React规约props在组件的生命周期内无法更改也不应改变。 在组件的生命周期中，随着该组件的props和state发生改变，组件的DOM表现也会有相应的变化。一个组件即是一个状态机，对于特定地输入，组件总返回一致的输出。 组件的生命周期可以分为三大过程。分别为： mounted组件被渲染为真实的DOM元素插入浏览器的DOM结构的一个过程。 update已经处于mounted状态的组件被重新渲染的过程。 unmounted处于mounted状态的组件被从浏览器DOM结构中移除的过程。 组件的完整生命周期如下： 在组件的生命周期中有几个比较重要的方法： getDefaultProps此方法返回的对象可以用于设置默认的props值。 getInitialState此方法用来初始化组件实例的state，在这个方法里可以访问组件的props变量。 componentWillMount此方法在组件首次渲染之前调用，是在调用render方法之前最后一次修改组件的机会。 render此方法会创建一个虚拟DOM，用来表示组件的输出。在组件中，render方法是必须的方法。render方法本身需要满足几点： 只能通过this.props和this.state访问组件的数据 可以返回null，false或者任何React组件 返回结果只能有一个顶级组件，不能返回一组元素 componentDidMount此方法在render方法执行之后被调用，所以在方法中可以获取组件在真实DOM的节点。 shouldComponentUpdate此方法决定组件是否重新渲染，如果方法返回结果为false的话，则组件不会调用render方法重新渲染。 componentWillUpdate此方法和componentWillMount类似，渲染后的组件在接收到新的props或者state改变之后，组件会调用此方法。 componentDidUpdate组件因接收到新的props或者state改变导致的重新渲染之后，会调用此方法，所以可以在方法中访问或者修改真实DOM。 componentWillUnmount当组件从DOM中卸载后销毁，会调用此方法完成所有的清理和销毁工作。在conponentDidMount中添加的任务都需要在此方法中销毁，比如创建的定时器和事件监听器。 React示例计时器 github源码 执行结果（原本想做个GIF，然而并不会做。。。）： React体验 使用单向数据流可以很好的隔绝业务，大大降低了单元测试的难度。 尽量将React组件（Component）做到小，做到细，也就是尽量拆分React组件。 基于数据驱动的方式开发，虽然开始的时候不容易理解，但确实可以减少前端的BUG。 React更适合数据驱动的项目，不太适应交互比较多的项目。 作者：王建双，就职于开鑫贷，主要负责Java Web方向的开发工作，也会根据兴趣涉猎前端相关的开发技术。目前在学习React和React Native、Spring高级特性。本人水平有限，欢迎各路大神就本文观点和出现的错误进一步的讨论。","categories":[],"tags":[{"name":"React","slug":"React","permalink":"http://kplxq.github.io/tags/React/"}],"keywords":[]}]}